{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "df0979c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "919a59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ESS8_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "17ef6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SD1', 'PO1', 'UN1', 'AC1', 'SC1',\n",
    "       'ST1', 'CO1', 'TR1', 'HD1', 'AC2', 'SC2', 'ST2',\n",
    "       'CO2', 'PO2', 'BE2', 'TR2', 'HD2']].iloc[range(3),:]\n",
    "X = X.to_numpy().T\n",
    "N, M = X.T.shape\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "716a419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "p = np.random.permutation(range(N))[:K]\n",
    "Z = X[:,p]\n",
    "# Z = np.random.randint(1, 6, (M,K)) \n",
    "\n",
    "#Q = Z.T @ Z\n",
    "#Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "R = X.T @ X\n",
    "Rt = torch.tensor(R,requires_grad=False).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "47234b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies both for eq. 7 and 8\n",
    "def error(a_i,Qt,qt):\n",
    "    return (0.5 * a_i.T @ Qt @ a_i) - (qt.T @ a_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "62f0513f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(4.1257, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(4.1257, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(47.6795, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(47.6795, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(109.4622, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(109.4622, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(174.1082, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(174.1082, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(369.6698, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(369.6698, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(47.2404, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(47.2404, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=0 tensor(29.1799)\n",
      "Loss before: tensor(103.3252, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(103.3252, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(43.9097, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(43.9097, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(100.2604, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(100.2604, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(1511.2260, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(1511.2260, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(645.7659, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(645.7659, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(1548.8223, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(1548.8223, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=1 tensor(27.0002)\n",
      "Loss before: tensor(6.8657, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(6.8657, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(3.4230, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(3.4230, grad_fn=<NormBackward0>) at iteration 0\n",
      "It took 1823 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(4.0673, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(4.0673, grad_fn=<NormBackward0>) at iteration 0\n",
      "It took 2365 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(839.8330, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(839.8330, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(831.3317, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(831.3317, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(1562.5132, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(1562.5132, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=2 tensor(25.2140)\n",
      "Loss before: tensor(11.8042, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(11.8042, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(5.3090, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(5.3090, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(48.4027, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(48.4027, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(121.8206, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(121.8206, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(402.1840, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(402.1840, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(2340.1052, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(2340.1052, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=3 tensor(16.5476)\n",
      "Loss before: tensor(96.1896, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(96.1896, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(16.4223, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(16.4223, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(59.6749, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(59.6749, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(1001.1598, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(1001.1598, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(38.2138, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(38.2138, grad_fn=<NormBackward0>) at iteration 0\n",
      "It took 8820 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(162.0874, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(162.0874, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=4 tensor(33.6443)\n",
      "Loss before: tensor(78.4157, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(78.4157, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(58.2616, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(58.2616, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(95.9124, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(95.9124, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(362.3035, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(362.3035, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(9350.8516, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(9350.8516, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(208.8048, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(208.8048, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=5 tensor(20.8745)\n",
      "Loss before: tensor(57.6506, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(57.6506, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(37.0840, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(37.0840, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(115.4764, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(115.4764, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(543.3840, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(543.3840, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(901.0410, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(901.0410, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(47.8392, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(47.8392, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=6 tensor(24.3378)\n",
      "Loss before: tensor(30.5527, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(30.5527, grad_fn=<NormBackward0>) at iteration 0\n",
      "It took 3554 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(53.3747, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(53.3747, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(30.0733, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(30.0733, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(14.8049, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(14.8049, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(891.9333, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(891.9333, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(529.2587, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(529.2587, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=7 tensor(29.1060)\n",
      "Loss before: tensor(54.5199, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(54.5199, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(80.0714, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(80.0714, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(42.8742, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(42.8742, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(629.6053, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(629.6053, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(216.5818, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(216.5818, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(190.1932, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(190.1932, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=8 tensor(41.1609)\n",
      "Loss before: tensor(118.4357, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(118.4357, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(530.0699, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(530.0699, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(12.4315, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(12.4315, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(197.6336, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(197.6336, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(198.4894, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(198.4894, grad_fn=<NormBackward0>) at iteration 0\n",
      "Loss before: tensor(175.2119, grad_fn=<NormBackward0>)\n",
      "Loss is tensor(175.2119, grad_fn=<NormBackward0>) at iteration 0\n",
      "RSS at n=9 tensor(23.7785)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "A = np.zeros((K,N))#.tolist()\n",
    "B = np.zeros((N,K))#.tolist()\n",
    "\n",
    "# LOOP UNTIL RSS IS LOW\n",
    "for n in range(10): #N\n",
    "    Q = Z.T @ Z\n",
    "    Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "    \n",
    "    # LOOP THROUGH ENTIRE A\n",
    "    for i in range(N):\n",
    "        q = Z.T @ X[:,i]\n",
    "        qt = torch.tensor(q,requires_grad=False).float()\n",
    "\n",
    "        a_i = torch.autograd.Variable(torch.rand(K, 1), requires_grad=True) # eller er det Kx1 ?\n",
    "\n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(a_i,Qt,qt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(10000): # 100000\n",
    "            Delta = error(a_i,Qt,qt) \n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            a_i.data -= step_size * a_i.grad.data # step\n",
    "            a_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "\n",
    "        A[:,i] = np.array(a_i.tolist()).flatten() \n",
    "        # print('Loss after: %s' % (torch.norm( error(a_i,Qt,qt) )))\n",
    "\n",
    "    ### apply softmax here? ###\n",
    "    \n",
    "    Z = X @ A.T @ np.linalg.inv(A@A.T)\n",
    "    \n",
    "    # LOOP THROUGH ENTIRE B\n",
    "    for i in range(K): #K\n",
    "        r = X.T @ Z[:,i]\n",
    "        rt = torch.tensor(r,requires_grad=False).float()\n",
    "\n",
    "        b_i = torch.autograd.Variable(torch.rand(N, 1), requires_grad=True) \n",
    "\n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(b_i,Rt,rt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(10000): # 100000\n",
    "            Delta = error(b_i,Rt,rt)\n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            b_i.data -= step_size * b_i.grad.data # step\n",
    "            b_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "\n",
    "        B[:,i] = np.array(b_i.tolist()).flatten() \n",
    "        #print('Loss after: %s' % (torch.norm( error(b_i,Rt,rt) )))    \n",
    "    \n",
    "    Z = X @ B\n",
    "    \n",
    "    Zt = torch.tensor(Z, requires_grad=False).float()\n",
    "    At = torch.tensor(A, requires_grad=False).float()\n",
    "    Xt = torch.tensor(X,requires_grad=False).float()\n",
    "    print(\"RSS at n=%s\" % n, torch.norm(Xt-Zt@At,p='fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f4f6bfbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-1ba307fc0bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "A[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbdba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21245b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ac3791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "200414b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:,1] = np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9aae0611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "291d0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(2, 2, requires_grad=False)\n",
    "b = torch.rand(2, 1,  requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3aedd449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1062, -1.4374],\n",
       "        [-2.7958,  3.7027]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inverse(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch37]",
   "language": "python",
   "name": "conda-env-torch37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
