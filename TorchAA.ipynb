{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df0979c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from timeit import default_timer as timer\n",
    "from scipy.special import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919a59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ESS8_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17ef6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SD1', 'PO1', 'UN1', 'AC1', 'SC1',\n",
    "       'ST1', 'CO1', 'TR1', 'HD1', 'AC2', 'SC2', 'ST2',\n",
    "       'CO2', 'PO2', 'BE2', 'TR2', 'HD2']].iloc[range(100),:]\n",
    "X = X.to_numpy().T\n",
    "N, M = X.T.shape\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "716a419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "p = np.random.permutation(range(N))[:K]\n",
    "Z = X[:,p]\n",
    "# Z = np.random.randint(1, 6, (M,K)) \n",
    "\n",
    "#Q = Z.T @ Z\n",
    "#Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "R = X.T @ X\n",
    "Rt = torch.tensor(R,requires_grad=False).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47234b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies both for eq. 7 and 8\n",
    "def error(a_i,Qt,qt):\n",
    "    return (0.5 * a_i.T @ Qt @ a_i) - (qt.T @ a_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec2091d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyConstrains(M):\n",
    "    return softmax(M, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62f0513f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(12.1962, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.1962, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(81.5849, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.5849, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(47.7395, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.7395, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 5406 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(10.3931, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.3931, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(39.5294, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.5294, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(9.2556, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.2556, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(90.6890, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.6890, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(54.3597, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.3597, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(79.4173, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.4173, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(54.8276, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.8276, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(1.6542, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.6542, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(19.2367, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.2367, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(103.9883, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.9883, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(45.1711, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.1711, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(42.2810, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.2810, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(2.8202, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.8202, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(32.2940, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.2940, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(36.7737, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.7737, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(12.9244, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.9244, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(95.4817, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(95.4817, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(88.8736, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.8736, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(84.1288, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(84.1288, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(0.6146, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.6146, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(90.5955, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.5955, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(17.9385, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17.9385, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(10.8740, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.8740, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(3.4189, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.4189, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(58.7542, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.7542, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(25.1637, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.1637, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(55.4943, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(55.4943, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(32.8516, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.8516, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(26.1072, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.1072, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(4.5457, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.5457, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 2884 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(78.2733, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(78.2733, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(99.9450, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(99.9450, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(41.9658, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.9658, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(49.6702, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.6702, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(81.0702, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.0702, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(48.7188, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.7188, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(37.5359, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.5359, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(91.0604, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(91.0604, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(48.8305, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.8305, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(55.6210, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(55.6210, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.3113, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.3113, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(9.4291, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.4291, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 1099 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(27.9979, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.9979, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(30.1737, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.1737, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(110.2757, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.2757, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(27.0091, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.0091, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(10.8919, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.8919, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(102.9357, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(102.9357, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(53.0081, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.0081, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(18.2269, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18.2269, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(0.1777, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.1777, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(8.5943, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8.5943, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(24.9464, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.9464, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(46.7552, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.7552, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(24.7570, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.7570, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(60.6119, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.6119, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(58.5717, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.5717, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 4229 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(1.7535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.7535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(68.3606, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.3606, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(51.7059, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.7059, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(22.9930, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.9930, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(31.0068, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.0068, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(61.7355, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.7355, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(48.9422, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.9422, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(28.6516, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(28.6516, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(53.7110, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.7110, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(50.1502, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.1502, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 8018 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(79.6441, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.6441, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(0.9404, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.9404, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(50.8483, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.8483, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 8217 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(109.3689, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(109.3689, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(42.7013, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.7013, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(89.5677, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(89.5677, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.8010, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.8010, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(150.9277, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(150.9277, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(33.7510, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.7510, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(26.8464, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.8464, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(65.7151, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.7151, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(18.2426, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18.2426, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(62.3488, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.3488, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(58.6009, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.6009, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(99.5323, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(99.5323, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(62.6920, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.6920, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(86.3591, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(86.3591, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(81.1580, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.1580, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(57.1068, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.1068, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(75.1730, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(75.1730, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(34.4970, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.4970, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(80.7221, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(80.7221, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(12.3807, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.3807, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(33.8832, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.8832, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(64.9737, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.9737, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(19.0758, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.0758, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(1.1658, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.1658, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.5163, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.5163, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(65.8900, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.8900, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(82.1845, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.1845, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(148778., grad_fn=<NormBackward1>)\n",
      "Loss for b is tensor(148778., grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 4152 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(164868.7344, grad_fn=<NormBackward1>)\n",
      "Loss for b is tensor(164868.7344, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(153406.0625, grad_fn=<NormBackward1>)\n",
      "Loss for b is tensor(153406.0625, grad_fn=<NormBackward1>) at iteration 0\n",
      "RSS at n=0 tensor(50.1622)\n",
      "Loss before: tensor(88.2199, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.2199, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(103.0967, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.0967, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(100.9868, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(100.9868, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.4360, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.4360, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(34.8916, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.8916, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(62.3053, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.3053, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 9974 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(83.5593, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.5593, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 9219 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(71.2664, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.2664, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(91.6058, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(91.6058, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(53.1590, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.1590, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 2847 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(80.5747, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(80.5747, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(52.0281, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.0281, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(21.8580, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.8580, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(60.0672, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.0672, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(124.5665, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(124.5665, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 7062 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(34.0109, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.0109, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(12.8770, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.8770, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 3892 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(28.1991, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(28.1991, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(6.7105, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.7105, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(94.3172, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.3172, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 2575 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(73.9592, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(73.9592, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(46.7343, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.7343, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(76.4621, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(76.4621, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(0.8491, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.8491, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(27.5236, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.5236, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(19.6249, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.6249, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.9854, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.9854, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(47.9199, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.9199, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(42.7426, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.7426, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 930 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(58.5071, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.5071, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(3.8124, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.8124, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(86.1826, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(86.1826, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(39.6422, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.6422, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.3259, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.3259, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(100.6518, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(100.6518, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(40.7675, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.7675, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(65.7789, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.7789, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(81.7078, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.7078, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(130.2244, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(130.2244, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 4622 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(41.6640, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.6640, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(77.6548, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(77.6548, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(0.8445, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.8445, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(66.1886, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(66.1886, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(86.4667, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(86.4667, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(32.4924, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.4924, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(31.0809, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.0809, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 4759 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(71.8310, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.8310, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(79.1997, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.1997, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(40.4537, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.4537, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 895 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(48.6589, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.6589, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(106.6301, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(106.6301, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(14.5806, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.5806, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 6516 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(63.8802, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(63.8802, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(24.2544, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.2544, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 7300 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(40.2835, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.2835, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(57.9053, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.9053, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(15.7497, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15.7497, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(51.0807, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.0807, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(72.6578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.6578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(58.4771, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.4771, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(46.7053, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.7053, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 3085 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(96.4530, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(96.4530, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(52.9565, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.9565, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(64.7826, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.7826, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(77.1444, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(77.1444, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(65.9858, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.9858, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(46.7832, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.7832, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(64.3282, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.3282, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(58.0627, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.0627, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(3.7468, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.7468, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(108.0120, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(108.0120, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(6.2418, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.2418, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(38.3462, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.3462, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(13.7262, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.7262, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(50.7281, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.7281, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.2277, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.2277, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(85.0352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(85.0352, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 9897 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(48.5267, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.5267, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(36.3097, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.3097, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(82.0998, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.0998, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 7888 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(79.0915, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.0915, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(21.7977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.7977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(26.2890, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.2890, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(53.4052, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.4052, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(105.9826, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(105.9826, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(44.0476, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.0476, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(112.7533, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(112.7533, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(79.0414, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.0414, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 6454 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(61.0673, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.0673, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(99.7711, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(99.7711, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(10.9256, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.9256, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(29.9712, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.9712, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(47.2490, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.2490, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(38.2065, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.2065, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(36.0891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.0891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(45.0027, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.0027, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 2268 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(58.7641, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.7641, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(85.4093, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(85.4093, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(42.3145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.3145, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 2223 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(84.9720, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(84.9720, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(152510.8125, grad_fn=<NormBackward1>)\n",
      "Loss for b is tensor(152510.8125, grad_fn=<NormBackward1>) at iteration 0\n",
      "It took 2619 iterations to achieve 1e-05 loss.\n",
      "Loss before: tensor(155924.0156, grad_fn=<NormBackward1>)\n",
      "Loss for b is tensor(155924.0156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(165168.2812, grad_fn=<NormBackward1>)\n",
      "Loss for b is tensor(165168.2812, grad_fn=<NormBackward1>) at iteration 0\n",
      "RSS at n=1 tensor(50.1568)\n",
      "It took: 500.9775534999999 seconds to finish running\n",
      "The best RSS value was tensor(50.1568)\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "RSS_values = list()\n",
    "A = np.zeros((K,N))#.tolist()\n",
    "B = np.zeros((N,K))#.tolist()\n",
    "\n",
    "# LOOP UNTIL RSS IS LOW\n",
    "for n in range(2): #N\n",
    "    Q = Z.T @ Z\n",
    "    Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "    # LOOP THROUGH ENTIRE A\n",
    "    for i in range(N):\n",
    "        q = Z.T @ X[:,i]\n",
    "        qt = torch.tensor(q,requires_grad=False).float()\n",
    "\n",
    "        a_i = torch.autograd.Variable(torch.rand(K, 1), requires_grad=True) # eller er det Kx1 ?\n",
    "\n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(a_i,Qt,qt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(10000): # 100000\n",
    "            Delta = error(a_i,Qt,qt) \n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            a_i.data -= step_size * a_i.grad.data # step\n",
    "            a_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss for a is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "        \n",
    "        A[:,i] = np.array(a_i.tolist()).flatten() \n",
    "        \n",
    "        # print('Loss after: %s' % (torch.norm( error(a_i,Qt,qt) )))\n",
    "\n",
    "    ### apply softmax here? ###\n",
    "    \n",
    "    A = applyConstrains(A)\n",
    "    Z = X @ A.T @ np.linalg.inv(A@A.T)\n",
    "    \n",
    "    # LOOP THROUGH ENTIRE B\n",
    "    for i in range(K): #K\n",
    "        r = X.T @ Z[:,i]\n",
    "        rt = torch.tensor(r,requires_grad=False).float()\n",
    "\n",
    "        b_i = torch.autograd.Variable(torch.rand(N, 1), requires_grad=True) \n",
    "\n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(b_i,Rt,rt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(10000): # 100000\n",
    "            Delta = error(b_i,Rt,rt)\n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            b_i.data -= step_size * b_i.grad.data # step\n",
    "            b_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss for b is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "\n",
    "        B[:,i] = np.array(b_i.tolist()).flatten() \n",
    "        #print('Loss after: %s' % (torch.norm( error(b_i,Rt,rt) )))    \n",
    "    \n",
    "    # apply softmax here\n",
    "    B = applyConstrains(B)\n",
    "    Z = X @ B\n",
    "    \n",
    "    Zt = torch.tensor(Z, requires_grad=False).float()\n",
    "    At = torch.tensor(A, requires_grad=False).float()\n",
    "    Xt = torch.tensor(X,requires_grad=False).float()\n",
    "    print(\"RSS at n=%s\" % n, torch.norm(Xt-Zt@At,p='fro'))\n",
    "    RSS_values.append( torch.norm(Xt-Zt@At,p='fro'))\n",
    "    \n",
    "end = timer()\n",
    "\n",
    "print(\"It took: {0} seconds to finish running\".format(end - start))\n",
    "print(\"The best RSS value was\", min(RSS_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adbd8a",
   "metadata": {},
   "source": [
    "### Time at different data sizes\n",
    "### Here we run 1000 inner loops and 10 in the outer\n",
    "#### For X = 3 x 17  \n",
    "##### It took: 17.8 seconds\n",
    "\n",
    "\n",
    "#### For X = 6 x 17  \n",
    "##### It took: 28.08 seconds\n",
    "\n",
    "\n",
    "#### For X = 12 x 17\n",
    "#### It took: 48 seconds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbdba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21245b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ac3791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "200414b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:,1] = np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9aae0611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "291d0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(2, 2, requires_grad=False)\n",
    "b = torch.rand(2, 1,  requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3aedd449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1062, -1.4374],\n",
       "        [-2.7958,  3.7027]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inverse(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
