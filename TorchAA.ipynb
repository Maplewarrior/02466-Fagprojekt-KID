{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0979c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from timeit import default_timer as timer\n",
    "from scipy.special import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919a59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ESS8_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17ef6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SD1', 'PO1', 'UN1', 'AC1', 'SC1',\n",
    "       'ST1', 'CO1', 'TR1', 'HD1', 'AC2', 'SC2', 'ST2',\n",
    "       'CO2', 'PO2', 'BE2', 'TR2', 'HD2']].iloc[range(1000),:]\n",
    "X = X.to_numpy().T\n",
    "N, M = X.T.shape\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47234b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies both for eq. 7 and 8\n",
    "def error(a_i,Qt,qt):\n",
    "    return (0.5 * a_i.T @ Qt @ a_i) - (qt.T @ a_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef6089",
   "metadata": {},
   "source": [
    "## Defining the helper functions applyConstraints and furthestSum\n",
    "\n",
    "applyConstrants ensures that the constraints of the problem are upheld i.e. $e_{ij} \\geq 0$ and $1^T \\textbf{e}_j = 1$\n",
    "\n",
    "\n",
    "furthestSum is an effective way to initialize the values of $\\textbf{b}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2091d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyConstrains(M):\n",
    "    return softmax(M, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8101bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def furthestSum(X):\n",
    "    # Choose a random point for initialization\n",
    "    idx = int(np.random.choice(range(0,N)))\n",
    "    x_j = X[:,idx]\n",
    "\n",
    "    j_news = list()\n",
    "    j_news.append(idx)\n",
    "\n",
    "    excluded = [idx]\n",
    "\n",
    "    # Loop over the K archetypes\n",
    "    for n in range(K):\n",
    "        best_val = 0\n",
    "        best_idx = 0\n",
    "        # Loop over all unseen samples\n",
    "        for i in range(N-len(excluded)):\n",
    "            if i not in excluded:\n",
    "                val = 0\n",
    "                # sum over each element for each point\n",
    "                for ele in j_news:\n",
    "                    for j in range(M):\n",
    "                        \n",
    "                        val += np.abs(X[j,i] - X[j , ele])\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_idx = i\n",
    "                    \n",
    "        # \n",
    "        j_news.append(int(best_idx))\n",
    "        excluded.append(best_idx)\n",
    "        # Remove the random initialization\n",
    "        if n == 0:\n",
    "            j_news.pop(0)\n",
    "            excluded.pop(0)\n",
    "        \n",
    "    return j_news\n",
    "\n",
    "\n",
    "#init_vals_b = X[init_idxs[i],:].astype(np.float64)\n",
    "#init_vals_bt = torch.tensor(init_vals_b, requires_grad = False).float()\n",
    "\n",
    "#init_vals_b.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8b223",
   "metadata": {},
   "source": [
    "## Initializing variables for AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "120d3834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1eb8ce3a5d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing Z through furthest sum\n",
    "#Z = X[:,furthestSum(X)]\n",
    "import random\n",
    "Z = X[:,random.sample(range(1, M), K)]\n",
    "print(Z.shape)\n",
    "R = X.T @ X\n",
    "Rt = torch.tensor(R,requires_grad=False).float()\n",
    "\n",
    "RSS_values = list()\n",
    "A = np.zeros((K,N))#.tolist()\n",
    "B = np.zeros((N,K))#.tolist()\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0513f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(294.8781, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(294.8781, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.4361, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.4361, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(311.6892, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(311.6892, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(204.3376, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(204.3376, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13.3575, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.3575, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.6479, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.6479, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(83.8687, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.8687, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(231.1296, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(231.1296, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(263.0223, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(263.0223, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(125.1446, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(125.1446, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24.6603, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.6603, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(171.7413, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(171.7413, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(167.2760, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(167.2760, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.1492, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.1492, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.6052, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.6052, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(156.5578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(156.5578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(122.0814, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(122.0814, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.1435, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.1435, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(156.5359, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(156.5359, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(71.8968, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.8968, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.7426, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.7426, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(76.3614, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(76.3614, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(179.9331, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(179.9331, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.3411, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.3411, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3.5769, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.5769, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(64.6324, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.6324, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(226.6951, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(226.6951, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.3665, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.3665, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(61.5815, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.5815, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(309.8242, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(309.8242, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.3132, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.3132, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(113.9006, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(113.9006, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(229.8954, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(229.8954, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.2307, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.2307, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.0201, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.0201, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(29.0085, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.0085, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(222.0619, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(222.0619, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(90.4032, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.4032, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(238.8572, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(238.8572, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(189.1313, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(189.1313, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.1362, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.1362, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.9372, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.9372, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(27.7177, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.7177, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(140.4506, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(140.4506, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(52.1034, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.1034, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(177.8923, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(177.8923, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(322.0264, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(322.0264, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.4009, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.4009, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(31.6135, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.6135, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(50.1581, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.1581, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(108.7547, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(108.7547, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(111.4284, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(111.4284, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(139.6869, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(139.6869, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.2199, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.2199, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(406.7508, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(406.7508, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19.7043, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.7043, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(39.9111, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.9111, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(145.9430, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(145.9430, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.6486, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.6486, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(56.4146, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.4146, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.5210, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.5210, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.8586, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.8586, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(313.0924, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(313.0924, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(451.9153, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(451.9153, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6.5207, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.5207, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(69.9999, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(69.9999, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(67.3833, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.3833, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13.0206, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.0206, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.8512, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.8512, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(61.4728, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.4728, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(113.9948, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(113.9948, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(247.0900, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(247.0900, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(57.1721, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.1721, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.4817, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.4817, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(133.9085, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(133.9085, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(128.0628, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(128.0628, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(119.5961, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(119.5961, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(138.2428, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(138.2428, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(280.4985, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(280.4985, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.8519, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.8519, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(68.9627, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.9627, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.0986, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.0986, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(275.2828, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(275.2828, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.1644, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.1644, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3.5971, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.5971, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1.5919, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.5919, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(81.2735, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.2735, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(68.7988, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.7988, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(inf, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "RSS_values = list()\n",
    "A = np.zeros((K,N))#.tolist()\n",
    "B = np.zeros((N,K))#.tolist()\n",
    "\n",
    "# LOOP UNTIL RSS IS LOW\n",
    "for n in range(2): #N\n",
    "    Q = Z.T @ Z\n",
    "    Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "    # LOOP THROUGH ENTIRE A\n",
    "    for i in range(N):\n",
    "        q = Z.T @ X[:,i]\n",
    "        qt = torch.tensor(q,requires_grad=False).float()\n",
    "        \n",
    "        if n == 0:\n",
    "            a_i = torch.autograd.Variable(torch.rand(K, 1), requires_grad=True) # eller er det Kx1 ?\n",
    "            optimizer_a = optim.SGD([a_i], lr=0.05)\n",
    "        \n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(a_i,Qt,qt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(100): # 100000\n",
    "            optimizer_a.zero_grad()\n",
    "            Delta = error(a_i,Qt,qt) \n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            optimizer_a.step()\n",
    "            #a_i.data -= step_size * a_i.grad.data # step\n",
    "            #a_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss for a is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "        \n",
    "        A[:,i] = np.array(a_i.tolist()).flatten() \n",
    "        \n",
    "        print('Loss after: %s' % (torch.norm( error(a_i,Qt,qt) )))\n",
    "    \n",
    "    A = applyConstrains(A)\n",
    "    Z = X @ A.T @ np.linalg.inv(A@A.T)\n",
    "    \n",
    "    \n",
    "    # LOOP THROUGH ENTIRE B\n",
    "    for i in range(K): #K\n",
    "        r = X.T @ Z[:,i]\n",
    "        rt = torch.tensor(r,requires_grad=False).float()\n",
    "        \n",
    "        if n == 0:\n",
    "            b_i = torch.autograd.Variable(torch.randn(N,1), requires_grad=True)\n",
    "            optimizer_b = optim.SGD([b_i], lr=0.05)\n",
    "        \n",
    "        \n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(b_i,Rt,rt)\n",
    "        #print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(10000): # 100000\n",
    "            optimizer_b.zero_grad()\n",
    "            Delta = error(b_i,Rt,rt)\n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            optimizer_b.step()\n",
    "            \n",
    "            # b_i.data -= step_size * b_i.grad.data # step\n",
    "            # b_i.grad.data.zero_()\n",
    "            #if k % 10000 == 0: print('Loss for b is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "\n",
    "        B[:,i] = np.array(b_i.tolist()).flatten() \n",
    "        #print('Loss after: %s' % (torch.norm( error(b_i,Rt,rt) )))    \n",
    "    \n",
    "    # apply softmax here\n",
    "    B = applyConstrains(B)\n",
    "    Z = X @ B\n",
    "    \n",
    "    print(n)\n",
    "    Zt = torch.tensor(Z, requires_grad=False).float()\n",
    "    At = torch.tensor(A, requires_grad=False).float()\n",
    "    Xt = torch.tensor(X,requires_grad=False).float()\n",
    "    print(\"RSS at n=%s\" % n, torch.norm(Xt-Zt@At,p='fro'))\n",
    "    RSS_values.append( torch.norm(Xt-Zt@At,p='fro'))\n",
    "    \n",
    "end = timer()\n",
    "\n",
    "print(\"It took: {0} seconds to finish running\".format(end - start))\n",
    "print(\"The best RSS value was\", min(RSS_values))\n",
    "print(RSS_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adbd8a",
   "metadata": {},
   "source": [
    "### Time at datasize 1000 x 17 for:\n",
    "#### Adam:\n",
    "Time in seconds: 351.60987780000005\n",
    "RSS: 160.3371\n",
    "#### SGD\n",
    "Time in seconds: \n",
    "RSS:\n",
    "#### Ordinal GD(?)\n",
    "Time in seconds: \n",
    "RSS:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbdba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21245b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0857a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419dfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f3e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2107acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7157e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc2b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cdd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1818264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
