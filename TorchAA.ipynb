{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df0979c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from timeit import default_timer as timer\n",
    "from scipy.special import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "919a59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ESS8_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17ef6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SD1', 'PO1', 'UN1', 'AC1', 'SC1',\n",
    "       'ST1', 'CO1', 'TR1', 'HD1', 'AC2', 'SC2', 'ST2',\n",
    "       'CO2', 'PO2', 'BE2', 'TR2', 'HD2']].iloc[range(1000),:]\n",
    "X = X.to_numpy().T\n",
    "N, M = X.T.shape\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47234b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies both for eq. 7 and 8\n",
    "def error(a_i,Qt,qt):\n",
    "    return (0.5 * a_i.T @ Qt @ a_i) - (qt.T @ a_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef6089",
   "metadata": {},
   "source": [
    "## Defining the helper functions applyConstraints and furthestSum\n",
    "\n",
    "applyConstrants ensures that the constraints of the problem are upheld i.e. $e_{ij} \\geq 0$ and $1^T \\textbf{e}_j = 1$\n",
    "\n",
    "\n",
    "furthestSum is an effective way to initialize the values of $\\textbf{b}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec2091d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyConstrains(M):\n",
    "    return softmax(M, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8101bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def furthestSum(X):\n",
    "    # Choose a random point for initialization\n",
    "    idx = int(np.random.choice(range(0,N)))\n",
    "    x_j = X[:,idx]\n",
    "\n",
    "    j_news = list()\n",
    "    j_news.append(idx)\n",
    "\n",
    "    excluded = [idx]\n",
    "\n",
    "    # Loop over the K archetypes\n",
    "    for n in range(K):\n",
    "        best_val = 0\n",
    "        best_idx = 0\n",
    "        # Loop over all unseen samples\n",
    "        for i in range(N-len(excluded)):\n",
    "            if i not in excluded:\n",
    "                val = 0\n",
    "                # sum over each element for each point\n",
    "                for ele in j_news:\n",
    "                    for j in range(M):\n",
    "                        \n",
    "                        val += np.abs(X[j,i] - X[j , ele])\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_idx = i\n",
    "                    \n",
    "        # \n",
    "        j_news.append(int(best_idx))\n",
    "        excluded.append(best_idx)\n",
    "        # Remove the random initialization\n",
    "        if n == 0:\n",
    "            j_news.pop(0)\n",
    "            excluded.pop(0)\n",
    "        \n",
    "    return j_news\n",
    "\n",
    "\n",
    "#init_vals_b = X[init_idxs[i],:].astype(np.float64)\n",
    "#init_vals_bt = torch.tensor(init_vals_b, requires_grad = False).float()\n",
    "\n",
    "#init_vals_b.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f4f2f",
   "metadata": {},
   "source": [
    "## Initializing variables for AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "120d3834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1eb8ce3a5d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing Z through furthest sum\n",
    "#Z = X[:,furthestSum(X)]\n",
    "import random\n",
    "Z = X[:,random.sample(range(1, M), K)]\n",
    "print(Z.shape)\n",
    "R = X.T @ X\n",
    "Rt = torch.tensor(X.T @ X,requires_grad=False).float()\n",
    "\n",
    "RSS_values = list()\n",
    "A = np.zeros((K,N))#.tolist()\n",
    "B = np.zeros((N,K))#.tolist()\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62f0513f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(300.8392, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(300.8392, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(323.7737, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2.2918, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.2918, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(126.4243, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(317.4768, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(317.4768, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(327.1076, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(207.6703, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(207.6703, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.7195, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12.1112, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.1112, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(204.7800, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.8335, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.8335, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(121.5551, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.4794, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.4794, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(127.3658, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(233.3780, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(233.3780, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(181.9111, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(269.5894, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(269.5894, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(272.9962, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.5340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.5340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.6642, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24.0180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.0180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(26.3600, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(174.4691, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(174.4691, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.0310, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(170.4958, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(170.4958, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.8691, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.9995, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.9995, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6.4335, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13.6783, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.6783, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.1489, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(158.9644, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(158.9644, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(29.2617, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(124.7060, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(124.7060, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(30.4754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(47.1205, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.1205, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14.8001, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(158.1862, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(158.1862, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.8561, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(72.1582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.1582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(280.2265, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.1285, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.1285, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(37.9436, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(76.5531, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(76.5531, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(278.2619, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(181.7386, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(181.7386, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5.6045, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22.5855, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.5855, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(201.1323, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3.1421, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.1421, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(96.5814, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(67.0054, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.0054, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(102.5160, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(231.5219, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(231.5219, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(121.3541, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.6422, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.6422, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(45.5745, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.0724, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.0724, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.7712, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(312.4100, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(312.4100, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(305.5062, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(39.1373, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.1373, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(172.2272, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(115.4153, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(115.4153, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.0214, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(236.0894, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.0894, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(120.0510, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.0912, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.0912, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(167.4601, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(57.8305, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.8305, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(268.3180, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.4888, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.4888, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(228.3906, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(226.8215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(226.8215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(132.2987, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(94.6180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.6180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(104.2191, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(240.4781, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(240.4781, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(155.4656, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(192.6837, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(192.6837, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.3494, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6.8694, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.8694, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(68.5113, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.3690, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.3690, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(34.5669, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.0424, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.0424, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(188.2417, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(143.8794, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(143.8794, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12.8306, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(53.0361, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.0361, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(207.4551, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(178.2894, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(178.2894, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6.3079, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(325.0724, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(325.0724, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(313.4388, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(127.5226, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(127.5226, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(42.0948, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.9076, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.9076, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(137.4123, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.3154, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.3154, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(51.7916, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(108.6993, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(108.6993, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(167.6154, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(114.2388, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(114.2388, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(51.9505, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(142.5853, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(142.5853, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.1600, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(90.1635, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.1635, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.6255, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(416.0146, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(416.0146, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(377.6321, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19.1731, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.1731, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(49.2235, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(38.9636, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.9636, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.9014, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(148.2383, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(148.2383, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(39.8185, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.9954, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.9954, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(177.0281, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(56.0112, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.0112, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(136.9943, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.2533, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.2533, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14.8696, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(52.7150, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.7150, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.9156, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(321.3312, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(321.3312, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(323.4052, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(457.5957, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(457.5957, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(393.1224, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6.5635, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.5635, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(64.0798, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(69.9169, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(69.9169, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(253.4501, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.9763, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.9763, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(89.2017, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13.6247, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.6247, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(181.7665, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.7840, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.7840, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(104.8493, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.6573, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.6573, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(51.6906, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(114.0989, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(114.0989, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(223.2567, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(250.8799, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(250.8799, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(179.2645, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(54.5456, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.5456, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(181.0833, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.2421, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.2421, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(261.2665, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(137.4319, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(137.4319, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(48.2526, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(130.1591, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(130.1591, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.0039, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(125.3851, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(125.3851, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(189.6095, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(142.4663, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(142.4663, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(44.8330, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(283.8973, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(283.8973, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(214.1637, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.9649, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.9649, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(161.0973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(71.7874, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.7874, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.7709, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.9820, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.9820, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.0052, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(278.5424, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(278.5424, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(247.7559, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.0844, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.0844, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(29.7820, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.7593, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.7593, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(113.8967, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.2345, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.2345, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10.6458, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(83.5380, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.5380, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(43.7663, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(71.6662, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.6662, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.8999, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(273.3813, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(273.3813, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(311.3777, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.6395, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.6395, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(56.7113, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.6649, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.6649, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(29.9627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(210.6697, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(210.6697, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.4940, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(147.4767, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(147.4767, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.4448, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(166.0071, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(166.0071, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(29.3687, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(310.3874, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(310.3874, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(292.8774, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.8057, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.8057, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(205.7693, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.1095, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.1095, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(257.5295, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(238.2631, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(238.2631, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(210.7515, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(153.2475, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(153.2475, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15.1951, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(110.0352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.0352, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3.3438, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(327.3164, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(327.3164, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(274.4658, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(52.6183, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.6183, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(147.5371, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(101.7783, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(101.7783, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(209.8845, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.7731, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.7731, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(151.8972, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(508.3243, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(508.3243, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(412.9361, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(83.4546, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.4546, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.2374, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(262.0041, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(262.0041, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(198.4446, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.6662, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.6662, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(68.0527, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(104.5047, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(104.5047, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(64.9159, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(362.1669, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(362.1669, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(350.3252, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(40.6715, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.6715, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(113.9408, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(300.8828, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(300.8828, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(249.2614, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(27.5854, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.5854, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(43.4145, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(73.7843, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(73.7843, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(264.2853, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(116.6175, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(116.6175, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18.2080, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6.5768, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.5768, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16.0441, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(154.0813, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(154.0813, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.6528, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(64.2212, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.2212, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(60.8386, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(101.3145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(101.3145, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.8898, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22.8276, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.8276, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(163.5975, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.3765, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.3765, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.9512, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(128.1186, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(128.1186, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.4851, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(156.8161, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(156.8161, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7.7702, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12.5245, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.5245, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(216.9883, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22.8705, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.8705, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.7607, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(47.0232, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.0232, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(191.4394, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.6858, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.6858, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5.4292, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(142.5215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(142.5215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.9713, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19.5003, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.5003, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(260.9601, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(55.5164, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(55.5164, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(236.3036, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(271.7261, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(271.7261, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(221.6175, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(83.5233, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.5233, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.9481, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(50.6248, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.6248, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.7032, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(79.1548, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.1548, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(70.3942, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(157.1652, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(157.1652, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(52.7198, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.0589, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.0589, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.8736, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.7679, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.7679, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.5949, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(79.9618, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.9618, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(268.9272, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(518.8565, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(518.8565, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(385.3080, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(484.7661, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(484.7661, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(389.9604, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23.8180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23.8180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(252.4301, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.0073, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.0073, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10.9080, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(247.8984, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(247.8984, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(162.3396, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(120.1029, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(120.1029, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.9199, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(171.7059, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(171.7059, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(71.4929, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.2455, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.2455, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(209.2016, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.5679, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.5679, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(75.6931, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(182.9128, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(182.9128, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(34.0566, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(252.8237, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(252.8237, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(209.0738, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.9133, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.9133, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(96.4744, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(199.1466, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(199.1466, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(116.3760, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.5358, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.5358, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21.7842, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(80.7500, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(80.7500, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(264.8437, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(223.6726, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(223.6726, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(69.4852, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(84.3026, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(84.3026, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(34.1522, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.3891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.3891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(134.7581, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.8369, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.8369, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(75.1061, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.0422, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.0422, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(223.2335, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(64.7309, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.7309, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(36.3731, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(63.8486, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(63.8486, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(159.1769, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(50.6173, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.6173, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(261.1051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(98.6415, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(98.6415, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(68.1028, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(177.9612, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(177.9612, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.1051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(354.1099, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(354.1099, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(331.1214, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.7445, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.7445, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(62.6331, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(186.7561, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(186.7561, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18.9794, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8.6146, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8.6146, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(121.6568, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(90.0214, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.0214, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(193.2316, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(53.5289, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.5289, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(142.6596, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(117.4147, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(117.4147, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(155.2890, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.4987, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.4987, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.8860, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(688.3848, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(688.3848, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(347.3168, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19.7030, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.7030, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(217.1913, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(244.9037, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(244.9037, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(188.3384, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(189.2496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(189.2496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(66.0555, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(189.3745, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(189.3745, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(49.8157, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(429.5985, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(429.5985, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(376.1882, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(196.9093, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(196.9093, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.6017, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(94.5467, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.5467, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(40.4540, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.6329, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.6329, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(82.9893, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(343.5237, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(343.5237, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(339.4283, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(95.2990, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(95.2990, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.6771, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(475.5524, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(475.5524, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(380.2969, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(26.5568, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.5568, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(255.1197, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.5226, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.5226, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(109.4389, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.2312, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.2312, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(250.0881, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.8312, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.8312, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(76.0619, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.1985, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.1985, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(28.1942, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24.1332, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.1332, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13.7122, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.1986, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.1986, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.5677, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(83.7215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.7215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(104.5795, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.5207, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.5207, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(112.8283, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.0614, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.0614, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13.0506, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(377.9245, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(377.9245, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(330.8713, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(153.6602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(153.6602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(235.8551, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(328.1022, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(328.1022, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(316.7910, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(101.9452, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(101.9452, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.0568, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(110.6584, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.6584, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.9877, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(118.4624, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(118.4624, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(60.4219, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(64.0534, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.0534, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(215.4249, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.3483, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.3483, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(164.9823, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24.0020, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.0020, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(51.6706, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(80.7052, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(80.7052, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(261.4765, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(180.1068, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(180.1068, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(127.1881, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(85.9677, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(85.9677, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(272.5156, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(91.2944, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(91.2944, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.9728, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(67.9072, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.9072, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(105.2222, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(165.9934, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(165.9934, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(28.3803, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3.2588, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.2588, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(158.2971, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(41.2809, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.2809, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(83.9956, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(108.7403, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(108.7403, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.3719, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.1271, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.1271, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(37.0007, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(151.7776, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(151.7776, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.3483, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(504.5159, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(504.5159, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(409.1031, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(351.5760, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(351.5760, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(334.6332, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(54.5439, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.5439, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(95.3857, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(38.6637, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.6637, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21.6936, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(266.8036, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(266.8036, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(162.4837, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(281.0125, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(281.0125, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(228.6942, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(115.0418, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(115.0418, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(157.3735, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(109.2415, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(109.2415, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(48.7599, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(358.0672, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(358.0672, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(325.6358, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.5626, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.5626, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(74.4973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(241.6442, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(241.6442, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(229.8191, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(48.5429, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.5429, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(60.1949, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(277.6829, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(277.6829, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(248.4539, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(145.5761, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(145.5761, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.0399, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.7953, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.7953, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(161.0893, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.9971, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.9971, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(209.4085, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.1969, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.1969, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3.6047, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(144.1648, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(144.1648, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.6965, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(389.5641, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(389.5641, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(357.2263, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(372.0417, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(372.0417, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(336.2386, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(96.8132, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(96.8132, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.5922, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(227.1380, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(227.1380, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(96.7173, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(536.8994, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(536.8994, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(409.6086, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(67.3988, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.3988, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(254.6629, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(151.8067, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(151.8067, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(36.0783, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(94.2581, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.2581, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.7576, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13.8201, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.8201, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13.9906, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19.2500, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.2500, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(218.9970, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(193.6577, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(193.6577, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(99.7156, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(376.7183, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(376.7183, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(333.4858, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(420.7881, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(420.7881, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(375.6782, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(41.4907, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.4907, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(115.7260, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(206.8041, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(206.8041, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(56.0901, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(229.8812, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(229.8812, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(257.3581, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(40.5552, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.5552, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(71.3131, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(246.7679, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(246.7679, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(103.2895, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.0865, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.0865, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(112.8176, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.4470, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.4470, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.6623, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(114.6398, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(114.6398, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(35.9058, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(87.3490, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.3490, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(99.7537, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.9958, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.9958, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(247.3627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(120.7319, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(120.7319, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(34.2589, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(419.9001, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(419.9001, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(368.4287, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(63.5431, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(63.5431, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(239.3333, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(195.1731, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(195.1731, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(96.1798, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(594.4207, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(594.4207, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(388.2336, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(140.2412, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(140.2412, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.7455, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.5130, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.5130, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(85.3663, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(250.7054, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(250.7054, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(139.3879, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(304.5532, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(304.5532, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(322.3528, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22.5875, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.5875, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(229.7732, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(53.3314, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.3314, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(190.5529, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23.6264, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23.6264, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(279.9077, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(176.9804, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(176.9804, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6.3456, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(320.5729, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(320.5729, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(296.9038, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.2058, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.2058, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.8326, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(31.6231, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.6231, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.0880, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(231.9749, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(231.9749, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(123.1829, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(89.2548, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(89.2548, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(190.8734, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.4104, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.4104, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.3922, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(43.2440, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(43.2440, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(239.8918, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.3895, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.3895, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.8905, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(485.1044, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(485.1044, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(387.4975, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(153.6078, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(153.6078, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5.9281, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.6273, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.6273, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(249.1709, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(328.9694, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(328.9694, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(279.5482, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.6307, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.6307, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(235.5812, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(73.7293, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(73.7293, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(274.8262, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.8463, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.8463, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(126.2405, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15.9155, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15.9155, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(86.9471, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.4488, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.4488, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(206.4923, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(113.1466, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(113.1466, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8.8352, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.7363, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.7363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(46.4425, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24.0256, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.0256, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15.2036, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(231.4613, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(231.4613, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(144.2032, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(26.3767, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.3767, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23.5412, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.4454, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.4454, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(145.8804, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(120.0392, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(120.0392, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22.4057, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(151.4342, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(151.4342, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(40.3204, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.0121, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.0121, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8.6690, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(340.5956, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(340.5956, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(334.4285, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(125.8043, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(125.8043, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.3136, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(193.3239, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(193.3239, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18.4681, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.6076, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.6076, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(114.4791, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(253.6370, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(253.6370, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(219.1776, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(84.1818, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(84.1818, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(242.0121, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(238.7857, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(238.7857, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(85.0118, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(162.5461, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(162.5461, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10.2906, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(57.5723, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.5723, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(34.4590, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(320.2125, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(320.2125, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(304.4769, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(393.4292, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(393.4292, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(361.9698, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(650.4510, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(650.4510, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(393.3941, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.4606, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.4606, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12.0445, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(170.6238, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(170.6238, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15.2690, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.3618, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.3618, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(221.5324, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(471.1597, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(471.1597, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(391.8181, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9.4203, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.4203, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(116.3824, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(325.2646, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(325.2646, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(290.5771, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(79.6797, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.6797, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(276.4080, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(163.1968, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(163.1968, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(123.2891, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(72.0345, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.0345, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.9496, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.1703, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.1703, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(210.6396, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(194.1088, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(194.1088, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.0334, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.1108, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.1108, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(176.3465, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(219.0030, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(219.0030, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(221.5368, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.3276, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.3276, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(230.5177, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.7298, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.7298, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(61.1364, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.6213, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.6213, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.8397, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(384.8559, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(384.8559, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(337.8323, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(31.0404, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.0404, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(74.0494, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(98.2951, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(98.2951, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(44.9866, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(123.6156, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(123.6156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.8674, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(121.8959, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(121.8959, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(60.2872, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.3822, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.3822, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(223.3360, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.0966, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.0966, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.8859, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(115.0507, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(115.0507, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(35.6435, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.7432, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.7432, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(121.4563, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(58.4047, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.4047, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(68.8001, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.8276, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.8276, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(170.1996, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(78.6160, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(78.6160, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(67.0841, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(219.0133, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(219.0133, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(45.8931, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.2834, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.2834, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.9636, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(186.4688, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(186.4688, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22.6221, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(306.0192, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(306.0192, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(256.0705, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.7801, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.7801, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(96.4938, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.1970, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.1970, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.4320, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.6010, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.6010, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(91.5098, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(231.7388, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(231.7388, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(121.6145, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(222.5702, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(222.5702, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(251.0340, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(164.6719, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(164.6719, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(273.7142, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(47.1999, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.1999, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4.3505, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.2392, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.2392, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(151.1488, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(146.5438, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(146.5438, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(39.7919, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(158.0045, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(158.0045, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.0026, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(92.7214, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(92.7214, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(62.5378, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(292.5724, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(292.5724, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(266.4558, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(406.1582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(406.1582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(372.0250, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(260.9449, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(260.9449, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(295.5758, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(347.2186, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(347.2186, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(337.9261, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.7578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.7578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(208.6165, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(410.1906, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(410.1906, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(365.4093, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(77.1544, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(77.1544, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.4599, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(58.5928, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.5928, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(205.4480, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(145.7462, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(145.7462, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.8349, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(109.7888, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(109.7888, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19.0516, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17.9849, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17.9849, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(126.9441, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.1236, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.1236, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.9161, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(84.1529, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(84.1529, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(269.0355, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.6796, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.6796, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.5264, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(78.4615, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(78.4615, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.0998, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(214.9109, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(214.9109, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(66.7551, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(209.3815, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(209.3815, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(95.8369, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.4527, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.4527, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(91.4632, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(85.4003, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(85.4003, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.8999, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(210.8691, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(210.8691, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.1165, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(171.4312, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(171.4312, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.0570, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(294.6130, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(294.6130, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(235.8695, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(363.2319, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(363.2319, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(330.6134, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(87.3720, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.3720, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(255.6365, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.9535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.9535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(79.2489, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(332.2212, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(332.2212, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(333.8198, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(187.7914, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(187.7914, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(49.9995, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(112.4291, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(112.4291, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.9962, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(41.8067, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.8067, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(83.0753, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1.1167, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.1167, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(223.9494, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(177.0859, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(177.0859, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.1487, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(115.8933, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(115.8933, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.7049, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1.5952, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.5952, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(240.6065, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(86.3269, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(86.3269, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(228.1895, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(115.3024, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(115.3024, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(107.4028, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(143.1650, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(143.1650, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(36.3096, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22.9594, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.9594, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(232.0472, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(197.3336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(197.3336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(169.2539, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(55.1465, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(55.1465, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(121.7348, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(83.8344, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.8344, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.1056, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(182.5916, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(182.5916, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(84.7615, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(52.1013, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.1013, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.3645, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.2655, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.2655, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(103.7950, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(130.6116, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(130.6116, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(51.8866, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.4476, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.4476, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(205.3671, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(90.9542, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.9542, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.5288, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(122.0135, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(122.0135, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(98.0315, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(82.1009, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.1009, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(92.6504, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(262.7042, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(262.7042, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(276.0817, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.8773, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.8773, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(173.6077, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.0367, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.0367, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(183.4172, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.2748, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.2748, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(158.7522, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9.7891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.7891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(74.8071, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(75.4203, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(75.4203, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(273.3295, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15.6973, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15.6973, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.4162, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.5947, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.5947, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(250.3518, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(29.3244, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.3244, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(289.0031, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(94.9943, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.9943, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.6169, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(142.7276, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(142.7276, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(44.5147, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24.6580, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.6580, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(255.7528, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12.5650, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.5650, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(90.6042, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(243.4485, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(243.4485, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(242.9032, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2.9888, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.9888, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.9124, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(138.4658, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(138.4658, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19.7457, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(221.8310, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(221.8310, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(82.8134, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(174.3449, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(174.3449, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12.9946, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(148.5609, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(148.5609, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(39.5097, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(429.6698, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(429.6698, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(380.9218, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(106.4853, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(106.4853, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(43.0205, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(197.1386, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(197.1386, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10.1080, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(156.2344, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(156.2344, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3.2349, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.5606, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.5606, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(185.6219, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(61.1957, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.1957, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(73.0916, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(192.7045, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(192.7045, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(103.5468, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(331.9802, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(331.9802, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(299.9857, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(619.5801, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(619.5801, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(414.5331, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(498.4881, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(498.4881, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(387.5010, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(358.9251, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(358.9251, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(325.2556, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(75.8499, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(75.8499, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(35.0896, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(94.2439, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.2439, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(28.4578, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(401.7304, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(401.7304, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(377.5697, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(181.7325, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(181.7325, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(66.5087, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.0934, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.0934, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(217.7743, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(204.6051, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(204.6051, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(116.3845, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(336.9496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(336.9496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(302.0625, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(78.9586, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(78.9586, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(161.9367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(29.7550, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.7550, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(230.8582, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(204.3212, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(204.3212, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.5754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(75.6063, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(75.6063, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(271.4427, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9.1306, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.1306, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(121.4419, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1.9786, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.9786, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(161.3047, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(72.8228, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.8228, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(88.1049, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(115.7916, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(115.7916, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(154.6359, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(325.2777, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(325.2777, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(287.8924, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(173.3000, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(173.3000, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13.8035, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.8620, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.8620, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(276.3571, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(178.4379, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(178.4379, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.1002, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(138.6203, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(138.6203, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.4451, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(69.3213, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(69.3213, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(280.0122, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(28.7982, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(28.7982, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(261.4180, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(39.3596, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.3596, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(182.8869, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23.7961, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23.7961, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(55.2025, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.9048, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.9048, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(37.1017, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(77.4624, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(77.4624, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(259.4437, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(135.9615, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(135.9615, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(35.7924, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(92.6987, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(92.6987, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(56.0377, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(602.3245, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(602.3245, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(407.2020, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(56.3145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.3145, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(185.7885, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.1360, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.1360, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.9449, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(98.4203, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(98.4203, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(67.2505, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.3093, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.3093, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(172.8455, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22.5230, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.5230, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.2627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.3667, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.3667, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(103.1961, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(43.4447, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(43.4447, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(64.9041, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(190.8044, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(190.8044, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(64.1188, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(77.5112, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(77.5112, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(258.7392, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(332.5614, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(332.5614, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(326.5629, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(52.2968, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.2968, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(132.9733, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(146.4401, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(146.4401, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(37.9709, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(95.7365, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(95.7365, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(35.8367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(140.9105, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(140.9105, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(40.2156, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(27.4128, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.4128, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(248.1452, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.2915, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.2915, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(108.2132, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(278.1075, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(278.1075, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(269.5810, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(175.3185, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(175.3185, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.7622, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(533.0519, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(533.0519, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(404.7673, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.5114, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.5114, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(208.1956, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(28.8638, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(28.8638, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.1045, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.6278, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.6278, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(202.0222, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(276.6953, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(276.6953, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(183.8815, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2.2397, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.2397, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(117.0925, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(177.9196, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(177.9196, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12.5755, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.5667, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.5667, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(133.0405, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.9297, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.9297, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(159.7192, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.6899, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.6899, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(256.8390, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(270.2229, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(270.2229, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(156.4594, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(85.8287, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(85.8287, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(49.4830, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(144.8154, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(144.8154, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18.0196, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.7925, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.7925, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.2924, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(384.5037, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(384.5037, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(345.7336, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6.8363, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.8363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(50.8774, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(143.1861, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(143.1861, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.6176, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(314.9114, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(314.9114, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(257.8047, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(197.0201, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(197.0201, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.8492, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(398.4214, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(398.4214, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(368.3814, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23.8959, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23.8959, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(234.4859, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(75.8778, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(75.8778, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(274.5277, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(75.4322, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(75.4322, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(48.5295, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2.0674, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.0674, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(194.0442, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(98.0475, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(98.0475, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.8550, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(161.0453, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(161.0453, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22.2181, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(241.5987, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(241.5987, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(214.2834, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.9057, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.9057, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4.7352, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(69.1747, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(69.1747, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(232.7883, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(394.6186, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(394.6186, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(351.4260, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(210.5619, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(210.5619, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(74.1247, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(179.5374, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(179.5374, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.7527, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(462.3271, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(462.3271, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(384.0681, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.1335, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.1335, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(177.8180, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(50.9394, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.9394, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15.6559, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(285.8696, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(285.8696, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(276.0712, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(52.0169, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.0169, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(143.6353, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(206.8574, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(206.8574, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(108.2681, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(175.0880, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(175.0880, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15.0101, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(211.0660, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(211.0660, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(64.4739, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(461.4645, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(461.4645, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(394.6947, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(374.9918, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(374.9918, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(340.5180, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.4266, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.4266, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(231.8648, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(221.0411, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(221.0411, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.1883, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(123.3074, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(123.3074, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(59.3721, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(145.4897, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(145.4897, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(72.7163, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(503.6710, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(503.6710, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(404.5128, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.4707, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.4707, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.7951, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(163.0724, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(163.0724, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.6468, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(106.0963, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(106.0963, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.8816, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.9662, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.9662, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.2840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(40.4983, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.4983, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(166.9915, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(224.3091, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(224.3091, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(91.5694, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(319.4965, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(319.4965, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(282.0988, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.4812, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.4812, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(111.6005, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.6040, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.6040, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(114.2490, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.6587, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.6587, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.0129, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.2110, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.2110, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(69.5540, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.5598, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.5598, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(256.1991, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.3045, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.3045, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.9767, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.8737, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.8737, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10.0727, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(74.9315, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(74.9315, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(279.8658, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(80.7854, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(80.7854, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.1137, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(81.8708, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.8708, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(62.8298, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(48.5909, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.5909, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(251.5640, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(511.1617, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(511.1617, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(401.0051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.4427, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.4427, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(155.4508, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(135.5403, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(135.5403, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(48.3526, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(67.5458, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.5458, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11.3036, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.0601, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.0601, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(161.3256, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(596.0657, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(596.0657, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(390.3196, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(293.2558, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(293.2558, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(268.0486, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(87.4790, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.4790, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(255.9416, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(164.4033, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(164.4033, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(75.6990, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(274.7791, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(274.7791, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(251.9691, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.9321, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.9321, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14.9432, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(193.5542, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(193.5542, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(61.8302, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(741.5762, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(741.5762, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(352.2867, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(322.1541, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(322.1541, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(278.7171, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(135.5035, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(135.5035, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(45.9264, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(66.5710, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(66.5710, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(225.6456, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.2361, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.2361, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(246.8128, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.2999, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.2999, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(114.2341, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.5891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.5891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(84.2488, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.1984, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.1984, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(99.2497, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.7430, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.7430, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(134.5154, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(519.9484, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(519.9484, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(374.1622, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(416.7772, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(416.7772, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(367.7893, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(156.5243, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(156.5243, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(30.6624, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.5811, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.5811, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(75.8688, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18.4453, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18.4453, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(254.4770, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.5792, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.5792, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(96.9935, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(192.8572, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(192.8572, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(44.7802, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13.2304, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.2304, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(46.2723, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.0126, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.0126, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11.6441, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(281.7203, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(281.7203, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(308.6913, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(324.5795, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(324.5795, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(305.8875, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(80.9658, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(80.9658, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(248.7290, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(72.0463, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.0463, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(29.4204, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(110.7896, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.7896, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(143.1915, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(256.7381, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(256.7381, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(134.2971, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(418.5063, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(418.5063, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(365.7711, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(277.6436, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(277.6436, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(199.6199, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.4147, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.4147, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(118.4196, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.9622, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.9622, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(214.7433, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(54.7214, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.7214, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(230.9399, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(250.8811, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(250.8811, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(296.2845, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12.2211, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.2211, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(63.6120, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(252.8246, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(252.8246, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(147.5453, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(97.2009, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(97.2009, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(213.6398, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(143.1182, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(143.1182, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(146.9317, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(412.1407, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(412.1407, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(373.2812, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(377.6529, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(377.6529, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(342.8918, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.2194, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.2194, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(97.9554, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.2187, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.2187, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(45.3673, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(189.9681, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(189.9681, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.2159, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(377.1621, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(377.1621, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(356.6892, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(191.8987, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(191.8987, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.7563, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.7464, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.7464, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(224.3526, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(155.8372, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(155.8372, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.3474, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.2583, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.2583, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(249.3678, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.6125, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.6125, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(75.4457, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(131.7169, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(131.7169, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25.8359, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(267.5433, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(267.5433, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(201.6528, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(132.2496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(132.2496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.3241, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.1452, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.1452, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22.8942, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(74.1448, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(74.1448, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(282.8315, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(163.8472, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(163.8472, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.0361, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24.5027, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.5027, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(132.4517, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(154.8812, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(154.8812, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(156.0564, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(131.9346, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(131.9346, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(161.1274, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.2920, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.2920, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(151.1029, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(101.0267, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(101.0267, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19.3116, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(312.5335, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(312.5335, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(314.6720, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.2577, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.2577, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(154.1443, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.6791, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.6791, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(118.5051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(143.5411, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(143.5411, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.4028, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(538.1196, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(538.1196, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(393.7427, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(95.9978, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(95.9978, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(59.6977, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(165.2749, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(165.2749, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(46.5587, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(190.4600, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(190.4600, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.3461, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(123.9382, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(123.9382, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.7075, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(131.0953, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(131.0953, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.6070, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17.9258, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17.9258, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.1273, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(292.2140, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(292.2140, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(257.5831, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(158.5626, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(158.5626, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(92.9814, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(167.3985, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(167.3985, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.5410, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9.1931, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.1931, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(158.7107, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(178.2509, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(178.2509, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25.3441, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(41.1611, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.1611, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(139.2979, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(38.7618, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.7618, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.4790, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(255.3884, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(255.3884, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(152.5285, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(146.4860, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(146.4860, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.0004, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(243.0223, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(243.0223, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(135.9456, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(90.6336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.6336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.2897, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23.8187, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23.8187, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.6390, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(87.2942, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.2942, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(251.6691, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.0433, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.0433, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(72.1105, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(238.9339, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(238.9339, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(247.1407, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(68.0567, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.0567, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6.2321, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(189.4032, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(189.4032, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9.4965, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(327.7209, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(327.7209, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(297.8414, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(90.6011, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.6011, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.8298, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.9259, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.9259, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(211.8641, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(62.0134, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(62.0134, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(62.2395, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(79.5272, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.5272, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12.6377, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(316.2340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(316.2340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(297.4474, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(175.9500, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(175.9500, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8.2697, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(607.7921, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(607.7921, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(393.4077, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(272.5468, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(272.5468, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(267.8531, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(250.0363, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(250.0363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(220.4276, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(149.6842, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(149.6842, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.0375, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.0623, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.0623, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(242.5536, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(170.0508, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(170.0508, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16.3401, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(191.7829, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(191.7829, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.7367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.5964, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.5964, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(65.0329, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.2463, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.2463, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(82.8879, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(56.5187, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.5187, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.9816, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(144.2297, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(144.2297, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.8759, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6.0675, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.0675, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.6051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.4060, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.4060, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.2376, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(78.6234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(78.6234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(266.1997, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12.6369, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.6369, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(194.2675, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13.6165, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.6165, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(183.8255, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(378.9157, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(378.9157, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(362.5840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.7231, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.7231, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(252.6683, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.9844, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.9844, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15.6413, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(28.8889, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(28.8889, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(56.2260, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(128.1757, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(128.1757, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.7085, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(94.2264, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.2264, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(62.6376, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(36.6210, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.6210, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(200.9942, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.0254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.0254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.0403, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.0648, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.0648, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(203.9792, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.6042, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.6042, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8.0677, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(163.2708, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(163.2708, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16.2920, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6.7136, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.7136, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.5562, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.4365, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.4365, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21.6607, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(237.3667, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(237.3667, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(163.7513, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(41.5535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.5535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.4396, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(92.7123, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(92.7123, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4.8007, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(459.1690, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(459.1690, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(400.4584, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.0516, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.0516, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(220.6901, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(283.1959, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(283.1959, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(199.8027, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(56.5057, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.5057, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(30.6907, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(71.0395, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.0395, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(265.8730, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(153.8468, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(153.8468, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.5352, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(244.1639, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(244.1639, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(180.5632, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.6077, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.6077, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.2884, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(82.0121, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.0121, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0721, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(63.6668, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(63.6668, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(198.5985, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.4352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.4352, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(48.1509, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(560.8973, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(560.8973, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(376.2005, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(41.1341, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.1341, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(36.1394, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(195.3568, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(195.3568, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(95.4011, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(39.0350, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.0350, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(151.3536, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.2781, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.2781, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.1264, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(148.2966, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(148.2966, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.6947, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(80.7283, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(80.7283, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(255.0196, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(78.0592, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(78.0592, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(37.7435, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(224.8532, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(224.8532, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(216.9472, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.1836, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.1836, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(104.0622, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15.1616, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15.1616, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(67.8650, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.9372, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.9372, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(59.0337, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(232.0758, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(232.0758, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(127.0258, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(237.6589, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(237.6589, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(152.2890, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(41.8655, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.8655, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(152.3227, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(256.3338, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(256.3338, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(259.8374, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.1931, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.1931, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(77.3995, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(188.8868, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(188.8868, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(113.5635, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(258.7122, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(258.7122, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(215.1204, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(82.8419, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.8419, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(265.6609, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.2790, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.2790, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.3223, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(181.1418, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(181.1418, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.8499, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(208.4379, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(208.4379, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(84.1044, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.0138, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.0138, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.3199, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(188.9868, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(188.9868, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(131.4573, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(56.4551, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.4551, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(186.0781, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.4346, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.4346, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(180.8557, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(66.7567, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(66.7567, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(186.0126, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(462.2733, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(462.2733, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(387.1731, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(260.3030, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(260.3030, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(171.1438, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.2180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.2180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(55.0761, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(171.2400, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(171.2400, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8.8711, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.9658, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.9658, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(79.9692, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.2477, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.2477, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(52.7303, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18.9770, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18.9770, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(97.3532, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.3270, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.3270, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(150.4806, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(110.2244, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.2244, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(61.1120, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(386.2636, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(386.2636, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(349.2963, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(76.3275, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(76.3275, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(263.5000, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(169.5692, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(169.5692, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13.2745, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.3200, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.3200, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(169.0975, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.6891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.6891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(231.5809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(194.9496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(194.9496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(35.2922, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(435.1478, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(435.1478, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(385.9081, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.1296, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.1296, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(55.7991, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(479.0445, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(479.0445, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(406.2206, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(159.0106, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(159.0106, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(153.6410, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(78.4402, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(78.4402, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(229.3697, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(61.2962, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.2962, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(236.9777, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(387.5074, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(387.5074, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(349.3127, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(28.4477, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(28.4477, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.8399, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(27.2344, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.2344, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(52.2973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(113.2489, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(113.2489, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.8290, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(224.0113, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(224.0113, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(81.5934, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.5027, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.5027, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(248.6883, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(31.8559, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.8559, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(43.0627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(86.3981, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(86.3981, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(119.0104, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(102.1383, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(102.1383, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(215.3141, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(134.7790, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(134.7790, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(211.1024, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(237.0827, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(237.0827, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(124.3197, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(331.9507, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(331.9507, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(306.2012, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(156.3654, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(156.3654, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(252.3967, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(84.6231, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(84.6231, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(268.8840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(306.0761, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(306.0761, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(271.1240, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(69.8237, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(69.8237, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(248.1862, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(324.9309, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(324.9309, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(265.1072, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.3880, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.3880, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16.3390, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(116.6862, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(116.6862, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.7394, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(134.6269, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(134.6269, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(49.2989, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.1161, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.1161, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(231.2047, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18.4393, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18.4393, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(103.9775, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(461.0492, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(461.0492, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(394.6713, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(183.6667, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(183.6667, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(52.2966, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(40.5398, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.5398, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9.3693, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(146.0127, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(146.0127, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.4824, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(215.6693, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(215.6693, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(74.3556, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.3574, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.3574, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.8461, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.5512, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.5512, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(161.9050, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.6143, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.6143, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(228.7979, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(159.7957, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(159.7957, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.5569, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.4802, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.4802, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(84.3807, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(416.1224, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(416.1224, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(375.4710, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(405.7021, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(405.7021, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(346.2774, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(26.1337, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.1337, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(116.6625, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(124.9110, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(124.9110, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(59.4757, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.8321, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.8321, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(229.1907, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(262.0849, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(262.0849, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(256.6511, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(221.3412, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(221.3412, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(50.3145, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(213.7026, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(213.7026, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(221.4896, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(79.3185, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.3185, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12.6802, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.6889, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.6889, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(35.6787, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(141.2005, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(141.2005, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23.1944, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(385.6000, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(385.6000, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(358.0474, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(121.0305, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(121.0305, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(45.5980, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(112.4551, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(112.4551, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(60.0045, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.4816, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.4816, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(52.5860, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.9724, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.9724, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(113.5090, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17.5266, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17.5266, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(82.5742, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(281.0787, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(281.0787, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(270.8851, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(40.1890, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.1890, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(56.5788, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(488.1984, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(488.1984, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(398.8577, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.7888, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.7888, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(50.4238, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(255.7483, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(255.7483, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(221.5117, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(213.9595, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(213.9595, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.6084, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(244.2889, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(244.2889, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(174.4984, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(237.0951, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(237.0951, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(141.9742, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(203.4164, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(203.4164, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(62.9771, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.8939, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.8939, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.2938, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9.1138, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.1138, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(91.2743, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(217.7339, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(217.7339, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(141.7108, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(40.1310, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(40.1310, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(116.4389, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(719.2738, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(719.2738, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(387.1623, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(511.5977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(511.5977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(419.8928, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.0715, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.0715, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.5173, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(298.3130, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(298.3130, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(230.3612, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12.4496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.4496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(179.0688, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.6577, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.6577, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(168.5423, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.1352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.1352, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(119.5160, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(146.3557, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(146.3557, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(39.3893, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(101.7866, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(101.7866, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(66.3379, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15.0720, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15.0720, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(76.5050, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(178.1483, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(178.1483, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(91.5585, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(48.5112, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.5112, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.3940, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(236.9676, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.9676, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(144.3464, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(420.5465, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(420.5465, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(383.1764, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(212.7490, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(212.7490, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(85.5733, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(39.9891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.9891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(85.3260, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(178.2234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(178.2234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7.1968, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(513.0703, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(513.0703, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(389.1737, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(112.0611, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(112.0611, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(36.5421, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(251.8178, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(251.8178, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(147.1057, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.5533, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.5533, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(135.6546, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(173.0955, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(173.0955, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(52.5397, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(58.3443, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.3443, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(203.6880, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(110.3793, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.3793, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5.7486, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(290.5023, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(290.5023, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(273.5104, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(64.6100, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.6100, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(28.7131, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(54.9909, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.9909, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(157.9184, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.2516, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.2516, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.1200, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(237.9596, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(237.9596, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(212.4116, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.2940, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.2940, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(60.5957, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(73.2033, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(73.2033, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(277.4962, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.9874, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.9874, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6.8858, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.5793, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.5793, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(233.7135, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(256.7364, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(256.7364, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(143.0444, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(122.6373, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(122.6373, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(51.5680, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(118.2039, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(118.2039, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.7685, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(111.9314, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(111.9314, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.8453, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(270.9476, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(270.9476, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(187.1667, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(293.4214, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(293.4214, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(232.0367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1.9307, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.9307, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14.9872, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(265.6158, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(265.6158, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(206.5669, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(269.6212, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(269.6212, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(144.8121, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(97.2238, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(97.2238, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21.9227, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(752.9551, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(752.9551, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(374.4417, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(159.6407, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(159.6407, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25.6710, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(50.3254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.3254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(79.3013, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.6378, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.6378, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(34.9248, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(196.8807, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(196.8807, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9.8540, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(212.5824, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(212.5824, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(236.1197, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.4501, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.4501, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(44.7503, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17.6838, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17.6838, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(80.0191, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(153.1392, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(153.1392, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.9200, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(94.9025, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(94.9025, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(151.5135, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(81.6628, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.6628, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(91.5400, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(152.3802, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(152.3802, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.5701, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(223.7377, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(223.7377, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(208.5567, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(226.1397, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(226.1397, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(184.7472, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(134.2220, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(134.2220, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.9825, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.8224, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.8224, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(208.4920, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.7505, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.7505, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(240.7773, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(270.1059, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(270.1059, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(202.3095, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(34.2169, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.2169, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(173.3436, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(267.9919, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(267.9919, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(231.4841, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(149.4993, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(149.4993, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.6849, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3.6798, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.6798, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(43.0790, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(70.1173, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(70.1173, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(69.2176, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(158.7899, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(158.7899, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.2001, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(191.6840, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(191.6840, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(142.0349, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(37.5172, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.5172, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(171.6620, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(114.8854, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(114.8854, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(206.1564, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(372.0175, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(372.0175, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(342.7598, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(26.7177, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.7177, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(211.6022, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.3115, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.3115, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(257.5033, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(570.0617, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(570.0617, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(392.8625, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(54.7057, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.7057, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(39.7972, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(171.1815, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(171.1815, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4.9391, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(44.4524, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.4524, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0090, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.8508, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.8508, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(44.9180, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(67.1830, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.1830, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(59.1107, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(63.4145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(63.4145, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(187.8940, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15.8268, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15.8268, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(75.4191, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(88.4254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(88.4254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(107.6565, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(47.0135, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.0135, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(186.7388, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(29.2387, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.2387, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6.2722, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(497.6894, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(497.6894, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(397.3784, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.4545, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.4545, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.0266, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(417.4145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(417.4145, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(365.3038, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12.8522, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12.8522, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(198.4694, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(58.9754, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.9754, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(209.4734, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.7202, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.7202, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(37.0806, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(71.0826, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.0826, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(267.0215, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(79.1014, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.1014, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(269.8074, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(240.5593, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(240.5593, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(139.7475, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(105.8381, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(105.8381, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(64.3888, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(108.5628, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(108.5628, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(61.1427, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.8929, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.8929, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(78.5046, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(264.9570, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(264.9570, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(164.8149, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(53.9270, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.9270, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(156.8031, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(208.7556, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(208.7556, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(88.1205, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(87.5910, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.5910, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(244.5339, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(42.8017, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.8017, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(59.7357, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(67.2006, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.2006, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25.0499, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(48.8321, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(48.8321, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22.4730, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(141.6693, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(141.6693, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(12.1018, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(107.0731, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(107.0731, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.6232, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.2149, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.2149, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(101.6754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1.9567, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.9567, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(154.9419, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(74.4352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(74.4352, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.0986, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(74.2021, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(74.2021, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(257.6781, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.9747, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.9747, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(178.4536, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(158.2156, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(158.2156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.1593, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(619.5616, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(619.5616, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(391.6939, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(123.9804, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(123.9804, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.8083, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(183.7976, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(183.7976, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(214.9154, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(272.1057, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(272.1057, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(159.3804, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(147.7379, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(147.7379, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(40.5924, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.5625, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.5625, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(31.5741, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(180.5465, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(180.5465, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11.3985, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(179.6769, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(179.6769, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.9758, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(244.2073, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(244.2073, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(136.3503, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(66.3529, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(66.3529, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(221.3824, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(480.6086, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(480.6086, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(409.7914, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.2243, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.2243, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10.4013, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(126.1809, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(126.1809, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(49.2483, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(97.1862, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(97.1862, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25.1185, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(54.7362, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.7362, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14.3177, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.8618, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.8618, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(111.8024, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(205.8488, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(205.8488, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(73.3316, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(99.5010, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(99.5010, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(211.3757, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(267.8720, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(267.8720, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(165.7060, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.2025, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.2025, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(118.7592, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.8386, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.8386, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(206.0976, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(341.4380, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(341.4380, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(345.7523, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(188.0827, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(188.0827, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19.0332, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(360.6947, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(360.6947, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(352.9140, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(47.9683, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.9683, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.0787, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(297.4371, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(297.4371, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(251.6383, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(122.5902, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(122.5902, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.1477, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10.1553, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10.1553, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(39.2140, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(412.3098, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(412.3098, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(372.4902, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(236.5422, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.5422, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(203.3745, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(39.9747, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.9747, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.3236, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(194.1986, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(194.1986, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(28.9490, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(110.5588, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.5588, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.7856, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(236.9171, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.9171, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(100.8037, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(334.9788, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(334.9788, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(351.6312, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(124.7765, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(124.7765, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.2913, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(74.1433, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(74.1433, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.4809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(266.6675, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(266.6675, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(203.0319, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(58.2409, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(58.2409, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.3309, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(128.1162, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(128.1162, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.8098, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(176.5647, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(176.5647, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(6.4988, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.7133, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.7133, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(168.1167, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(38.1447, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.1447, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(125.1247, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.6998, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.6998, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(106.6620, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(217.4122, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(217.4122, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(224.5769, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(55.9161, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(55.9161, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.0556, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(83.5261, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(83.5261, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9.9852, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(105.0255, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(105.0255, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(48.8377, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20.6161, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.6161, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(55.5533, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(68.8136, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.8136, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.6398, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(205.9365, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(205.9365, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(164.6292, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.5124, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.5124, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.3554, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(241.2871, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(241.2871, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(138.5339, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(51.1355, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(51.1355, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(158.5969, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(212.5146, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(212.5146, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(87.6189, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(167.2225, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(167.2225, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(235.5801, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(122.9356, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(122.9356, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(54.8750, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(134.7230, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(134.7230, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.3089, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(314.8040, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(314.8040, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(269.2845, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.9668, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.9668, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(66.1750, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(258.7454, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(258.7454, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(233.9193, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(181.3783, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(181.3783, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.8335, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(254.2672, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(254.2672, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(162.4280, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(91.2063, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(91.2063, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20.1788, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(326.1377, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(326.1377, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(325.5942, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(233.2755, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(233.2755, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(108.7673, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(236.0591, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.0591, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(178.5095, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(71.1019, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.1019, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.4954, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(260.7184, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(260.7184, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(184.4894, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17.8145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17.8145, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(99.2627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.6475, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.6475, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(43.2903, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.0595, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.0595, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.0053, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.3558, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.3558, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(97.7900, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(271.9807, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(271.9807, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(235.2521, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(93.8382, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(93.8382, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(210.6382, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(148.6684, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(148.6684, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.5400, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.4315, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.4315, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(44.0805, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(101.0607, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(101.0607, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(216.9651, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(102.9688, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(102.9688, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.2150, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(167.6013, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(167.6013, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(67.5784, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(114.8715, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(114.8715, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(142.1073, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(102.4857, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(102.4857, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4.5192, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(399.7029, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(399.7029, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(373.6317, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(128.5335, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(128.5335, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(47.9395, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(193.2931, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(193.2931, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(158.8771, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(274.8938, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(274.8938, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(209.9513, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(332.1171, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(332.1171, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(295.7697, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.4817, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.4817, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(79.2947, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(47.0681, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(47.0681, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(30.6035, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(198.0891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(198.0891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(82.5894, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(26.0636, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(26.0636, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.2021, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(259.0878, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(259.0878, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(192.2755, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3.4306, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.4306, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(81.4804, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.1935, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.1935, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(207.1128, grad_fn=<CopyBackwards>)\n",
      "Loss for b is tensor(11820.3984, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(18671.7012, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(25377.2363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(11065.4854, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(107335.8125, grad_fn=<NormBackward1>) at iteration 0\n",
      "0\n",
      "RSS at n=0 tensor(160.3343)\n",
      "Loss before: tensor(191.3541, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(191.3541, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(604.0483, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(478.6805, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(478.6805, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(888.6448, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(845.5539, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(845.5539, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(973.8585, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1247.3643, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1247.3643, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(131.3221, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(149.3199, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(149.3199, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1153.5586, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1129.4869, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1129.4869, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1943.3390, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1745.3687, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1745.3687, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(348.4608, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(414.2452, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(414.2452, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2602.1343, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2667.0439, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2667.0439, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(737.1232, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(672.9259, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(672.9259, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2421.5593, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2300.6138, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2300.6138, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2374.4363, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2443.6284, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2443.6284, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(650.5095, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(534.8535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(534.8535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4555.1245, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4870.8755, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4870.8755, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(522.7667, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(643.3186, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(643.3186, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2318.4395, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2865.2485, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2865.2485, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5770.2051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5916.0654, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5916.0654, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(209.2514, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(135.9199, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(135.9199, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3581.6196, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3604.8013, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3604.8013, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6599.5557, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6903.4463, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6903.4463, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(780.9014, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(742.2177, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(742.2177, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2201.4048, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2153.9868, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2153.9868, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7588.6685, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8322.0371, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8322.0371, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4177.3770, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4261.4692, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4261.4692, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.9400, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(66.5068, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(66.5068, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5639.7930, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5549.5596, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5549.5596, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8739.3447, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8909.9795, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8909.9795, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2763.4841, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2838.9248, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2838.9248, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(315.1347, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(338.6760, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(338.6760, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6288.6040, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6166.5918, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6166.5918, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9434.6689, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9226.9902, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9226.9902, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3676.2207, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3915.4858, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3915.4858, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.0257, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25.7724, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.7724, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4589.4507, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4858.2622, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4858.2622, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10224.8076, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10408.0811, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10408.0811, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7174.8965, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6767.6592, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6767.6592, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(505.8758, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(524.2030, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(524.2030, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1797.4524, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(1662.5623, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1662.5623, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8064.9673, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8481.1064, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8481.1064, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10704.3027, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10909.2988, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10909.2988, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4873.1021, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4335.2690, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4335.2690, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(92.7822, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(31.9035, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.9035, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3265.7217, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3351.3818, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3351.3818, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9743.6094, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9895.5146, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9895.5146, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10623.1689, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9864.0234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9864.0234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3789.2012, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3919.5627, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3919.5627, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(40.0088, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(60.5087, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.5087, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3279.4678, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2885.4341, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2885.4341, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9041.9658, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9915.3516, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9915.3516, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11474.9590, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11409.9238, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11409.9238, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5505.5581, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4993.6836, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4993.6836, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(29.6448, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(179.0357, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(179.0357, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2146.6072, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2239.9712, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2239.9712, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8778.7627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8617.3223, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8617.3223, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11981.6211, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11825.4424, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11825.4424, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7761.2725, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8026.8701, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8026.8701, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1358.3674, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1341.6880, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1341.6880, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(590.8374, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(633.5475, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(633.5475, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6320.2593, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6119.1094, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6119.1094, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11782.8379, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11788.5557, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11788.5557, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11137.8184, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11263.0674, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11263.0674, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4575.4229, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4258.6982, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4258.6982, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(61.2185, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11.9922, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11.9922, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2606.0010, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2427.9473, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2427.9473, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8844.4326, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9381.0713, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9381.0713, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13225.0889, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13034.3018, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13034.3018, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9552.9111, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9448.9160, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9448.9160, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2442.2952, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2498.1660, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2498.1660, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(36.8495, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(81.6590, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.6590, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4614.4404, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4605.2476, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4605.2476, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11255.3584, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10611.3994, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10611.3994, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12815.6172, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13605.3916, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13605.3916, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8700.5908, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8296.9824, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8296.9824, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1544.7595, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1421.3215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1421.3215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(363.1688, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(286.0566, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(286.0566, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5203.3213, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5266.9580, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5266.9580, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11738.6016, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12298.0527, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12298.0527, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14064.3506, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13331.4561, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13331.4561, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8137.8643, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8031.2129, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8031.2129, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1458.2240, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1637.7073, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1637.7073, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(276.9551, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(270.7985, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(270.7985, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5254.6548, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6000.0840, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6000.0840, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12854.3926, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12123.2930, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12123.2930, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(14085.3672, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14511.5332, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14511.5332, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9412.7764, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8485.3584, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8485.3584, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1733.7052, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2010.5321, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2010.5321, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(142.8842, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(137.3105, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(137.3105, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4689.0786, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4866.1895, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4866.1895, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11750.8516, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11973.3525, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11973.3525, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15019.2324, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14973.6113, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14973.6113, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10868.1221, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10616.5391, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10616.5391, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3229.5398, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3021.7759, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3021.7759, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(26.7157, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.0590, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.0590, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3531.6211, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3697.4578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3697.4578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10712.4277, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10331.7900, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10331.7900, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14755.5059, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14776.9453, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14776.9453, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12615.0010, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12607.8174, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12607.8174, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5456.6157, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5216.2661, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5216.2661, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(183.9726, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(205.9399, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(205.9399, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1658.6118, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1688.6183, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1688.6183, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8089.1646, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7512.0469, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7512.0469, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13437.0850, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13524.1582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13524.1582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14310.7529, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15261.3838, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15261.3838, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9406.2598, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9033.0449, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9033.0449, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1943.6301, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1707.9077, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1707.9077, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(293.1201, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(132.7171, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(132.7171, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4631.8555, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4708.1938, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4708.1938, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11700.0391, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11611.4785, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11611.4785, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15503.0352, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15494.4023, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15494.4023, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12728.7754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12829.1816, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12829.1816, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5438.5215, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5126.7905, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5126.7905, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(157.0286, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(287.7817, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(287.7817, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1533.4814, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1786.6445, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1786.6445, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8394.3076, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8206.6924, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8206.6924, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14548.3027, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13373.6855, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13373.6855, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14553.3574, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15398.1025, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15398.1025, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10157.1738, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10282.1387, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10282.1387, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2868.3740, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2835.1726, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2835.1726, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(55.2610, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(33.7410, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.7410, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3693.6204, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3537.5781, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3537.5781, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10548.1406, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10709.9141, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10709.9141, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15883.7012, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16127.7666, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16127.7666, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15064.3984, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14861.4336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14861.4336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7966.3735, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7614.5420, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7614.5420, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1124.9478, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1340.1836, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1340.1836, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(410.4117, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(466.6929, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(466.6929, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5784.1670, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5846.9858, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5846.9858, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13089.9980, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13602.1846, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13602.1846, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(17392.9902, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16634.5234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16634.5234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13541.9648, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13680.8008, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13680.8008, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6009.8511, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5723.8340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5723.8340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(347.4109, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(485.0126, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(485.0126, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(883.3464, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1423.2986, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1423.2986, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7865.0776, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7610.1777, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7610.1777, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14660.2520, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14548.8857, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14548.8857, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17210.5684, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17503.4531, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17503.4531, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13259.6367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13369.6738, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13369.6738, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5280.5332, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4981.6851, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4981.6851, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(94.9169, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(199.9984, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(199.9984, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1734.6130, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1760.7778, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1760.7778, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8381.0273, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8338.6279, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8338.6279, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15319.4287, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15205.2568, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15205.2568, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17606.1191, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17561.9258, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17561.9258, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13170.2549, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13074.0781, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13074.0781, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5139.7285, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5477.9111, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5477.9111, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(226.0590, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(224.8959, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(224.8959, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1583.1123, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1476.3191, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1476.3191, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7904.8193, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8342.6221, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8342.6221, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15627.8672, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15498.8760, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15498.8760, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18389.3809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18320.1074, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18320.1074, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14317.7402, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14318.4150, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14318.4150, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6183.6919, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6083.0332, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6083.0332, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(412.8785, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(431.7100, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(431.7100, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1205.9926, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1027.7966, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1027.7966, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6946.3042, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7197.0107, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7197.0107, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14673.2637, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15208.1143, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15208.1143, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19143.9844, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18959.4980, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18959.4980, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16018.6270, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16389.7363, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16389.7363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8238.1875, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8169.7744, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8169.7744, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1206.3418, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(968.2379, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(968.2379, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(736.0111, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(384.9590, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(384.9590, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5589.9761, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6024.6855, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6024.6855, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13820.2246, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13734.6807, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13734.6807, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18885.0293, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19366.4316, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19366.4316, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18113.0098, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17838.3281, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17838.3281, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10533.1846, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10851.0996, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10851.0996, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2804.8574, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2703.9202, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2703.9202, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9.1185, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14.5564, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14.5564, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4326.6035, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3889.6448, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3889.6448, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11413.0576, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11886.1865, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11886.1865, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18476.1270, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17172.8301, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17172.8301, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18294.5762, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19037.7637, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19037.7637, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13637.3535, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13844.1328, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13844.1328, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5406.4321, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(5306.8555, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5306.8555, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(210.6825, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(222.7522, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(222.7522, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1565.7910, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1620.6101, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1620.6101, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8177.0420, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8199.7949, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8199.7949, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15810.2607, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15816.4912, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15816.4912, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19794.6816, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19486.5645, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19486.5645, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16894.2051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17034.6250, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17034.6250, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9239.8662, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8883.7930, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8883.7930, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1732.4257, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2273.8794, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2273.8794, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(83.1032, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(109.1182, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(109.1182, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4498.6709, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4625.2900, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4625.2900, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12412.1719, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12402.8594, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12402.8594, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18845.8594, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18948.6270, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18948.6270, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19901.8672, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19881.7402, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19881.7402, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14161.9248, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14332.5430, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14332.5430, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5652.1758, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5178.7783, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5178.7783, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(205.5979, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(236.6519, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.6519, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1568.3713, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1294.9717, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1294.9717, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7537.7349, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7735.0522, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7735.0522, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15466.8379, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15884.6582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15884.6582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20469.0254, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21505.7363, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21505.7363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19473.1367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17684.4434, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17684.4434, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10177.4805, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10641.7754, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10641.7754, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2750.1821, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2725.8970, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2725.8970, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(51.4303, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.5028, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.5028, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3366.9795, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3501.0640, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3501.0640, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11012.5928, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11306.3711, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11306.3711, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18516.2148, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18516.9316, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18516.9316, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20933.4336, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20390.8965, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20390.8965, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16102.3086, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16886.8906, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16886.8906, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8211.9072, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8034.8154, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8034.8154, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1177.2869, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1170.1010, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1170.1010, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(439.6584, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(652.7420, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(652.7420, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6256.9297, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6271.4995, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6271.4995, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14392.2305, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13763.7539, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13763.7539, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19761.5723, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19380.4863, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19380.4863, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19758.9512, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20038.6230, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20038.6230, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14066.0527, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13898.1172, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13898.1172, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5418.6338, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5924.6514, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5924.6514, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(405.6031, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(267.0861, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(267.0861, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1413.3882, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1223.9070, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1223.9070, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7337.7910, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7497.1470, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7497.1470, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15410.8027, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15339.4209, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15339.4209, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20520.3164, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20710.1484, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20710.1484, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19808.5684, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19474.3262, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19474.3262, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(12585.6865, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12931.7666, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12931.7666, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4412.9971, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4816.4639, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4816.4639, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(45.2115, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.8165, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.8165, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2252.6968, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2352.6311, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2352.6311, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9485.3633, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8880.1006, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8880.1006, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16648.4316, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16595.1094, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16595.1094, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20978.9375, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21814.9688, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21814.9688, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19793.5059, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19453.8789, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19453.8789, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11742.3008, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12049.7715, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12049.7715, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3595.4744, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3168.0068, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3168.0068, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(93.9551, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(68.6792, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.6792, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2969.9194, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3002.6069, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3002.6069, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10385.4150, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9979.7344, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9979.7344, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17515.1328, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17795.5078, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17795.5078, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21521.9785, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21157.5312, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21157.5312, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18429.8281, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18445.6250, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18445.6250, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10588.2217, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10739.0801, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10739.0801, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2829.2935, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2741.5803, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2741.5803, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16.9522, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(97.5970, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(97.5970, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3058.4873, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3417.7336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3417.7336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10977.9443, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10931.2314, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10931.2314, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18486.4805, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19025.4980, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19025.4980, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22400.2305, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21432.5156, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21432.5156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18174.4121, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18178.9941, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18178.9941, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10049.2715, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10741.3193, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10741.3193, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2725.4629, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2582.2625, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2582.2625, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0232, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(0.0241, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.0241, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3930.2207, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3820.9258, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3820.9258, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11492.3252, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12165.4336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12165.4336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19738.3848, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19735.7539, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19735.7539, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22658.5020, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20967.3340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20967.3340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17325.0371, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18209.5625, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18209.5625, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9851.8857, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9421.9092, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9421.9092, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2050.2581, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2356.2349, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2356.2349, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(26.9727, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(46.7487, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.7487, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3745.3008, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3878.9844, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3878.9844, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11570.6973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12221.4512, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12221.4512, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19760.3262, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18962.0312, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18962.0312, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21903.9688, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22133.9297, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22133.9297, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18495.9863, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18316.4590, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18316.4590, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9955.9004, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10375.5713, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10375.5713, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2488.9243, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2556.0474, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2556.0474, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6.9588, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9.9437, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.9437, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4120.2534, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3745.5515, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3745.5515, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11349.5674, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12116.0391, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12116.0391, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19772.5273, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19032.3320, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19032.3320, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22184.1621, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22905.9219, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22905.9219, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19415.7227, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(19384.2402, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19384.2402, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10842.7881, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10358.4014, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10358.4014, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2503.5981, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2610.8252, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2610.8252, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(30.1508, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3.5160, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.5160, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3779.8877, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3672.3621, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3672.3621, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11316.2539, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11562.1992, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11562.1992, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19230.5566, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17679.0059, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17679.0059, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21127.6504, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22584.7617, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22584.7617, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19708.5527, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18900.9453, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18900.9453, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10970.9141, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11514.5508, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11514.5508, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3289.8042, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3234.7314, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3234.7314, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(71.0791, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(68.9420, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.9420, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2955.2251, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3018.7317, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3018.7317, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10418.7363, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9710.7471, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9710.7471, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17359.1855, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17824.4434, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17824.4434, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22112.3398, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22629.2480, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22629.2480, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20586.5117, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19706.9766, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19706.9766, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12197.0195, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13008.0059, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13008.0059, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4318.1865, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4367.7998, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4367.7998, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3.2942, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2.9456, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.9456, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2152.9807, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2382.9473, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2382.9473, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9550.7695, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8204.5811, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8204.5811, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16028.0928, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17657.4395, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17657.4395, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22889.0898, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23392.6816, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23392.6816, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22308.5781, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22027.7578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22027.7578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14685.2773, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14092.7734, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14092.7734, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5168.7881, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5572.5830, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5572.5830, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(282.0833, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(203.2190, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(203.2190, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1574.2103, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1527.2783, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1527.2783, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8077.5830, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8013.7632, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8013.7632, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16314.9639, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16109.5098, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16109.5098, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22003.7227, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22797.7480, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22797.7480, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22924.2656, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22761.0977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22761.0977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16352.0508, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16420.6660, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16420.6660, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7139.7559, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6466.2671, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6466.2671, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(610.7101, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(769.0483, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(769.0483, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(729.6808, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(637.6173, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(637.6173, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6166.3467, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6486.6353, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6486.6353, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14809.0039, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14626.2979, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14626.2979, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21395.4609, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22319.6758, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22319.6758, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23892.4141, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22607.0254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22607.0254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17570.8945, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18102.9668, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18102.9668, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9106.0234, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9205.2598, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9205.2598, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1787.1409, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1811.3928, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1811.3928, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(163.1642, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(189.4304, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(189.4304, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4868.2637, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4623.2095, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4623.2095, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12582.0068, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13065.9150, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13065.9150, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(20689.4688, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20895.6406, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20895.6406, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23999.4141, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23632.1367, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23632.1367, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20046.2188, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20391.7598, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20391.7598, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11718.2607, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10686.4492, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10686.4492, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2768.6550, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3135.5273, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3135.5273, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(74.2200, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(102.2881, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(102.2881, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2853.6060, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2912.4507, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2912.4507, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10314.5264, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10488.8633, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10488.8633, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18627.2832, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19433.6621, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19433.6621, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24206.3281, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24262.9863, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24262.9863, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22530.3867, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20909.9277, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20909.9277, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13488.1104, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14463.5508, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14463.5508, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5390.8223, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5082.3433, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5082.3433, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(154.4490, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(222.3599, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(222.3599, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1523.5747, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1672.9454, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1672.9454, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8380.3320, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8071.7305, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8071.7305, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16366.2461, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15959.6006, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15959.6006, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21956.5625, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22414.8594, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22414.8594, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22922.0078, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23077.0742, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23077.0742, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17142.7363, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17257.3438, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17257.3438, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8060.4185, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7897.1895, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7897.1895, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1156.0160, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(996.1536, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(996.1536, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(610.5039, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(442.2198, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(442.2198, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5553.6538, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5716.0835, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5716.0835, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13895.9707, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13887.7520, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13887.7520, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21065.2754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20357.8027, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20357.8027, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22799.3027, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22999.1641, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22999.1641, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19049.3984, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18764.5996, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18764.5996, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10367.2500, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10976.6582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10976.6582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2909.2363, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2475.2075, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2475.2075, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(75.7589, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(95.9145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(95.9145, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3730.4429, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3266.7419, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3266.7419, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10599.3184, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10437.6553, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10437.6553, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18121.5352, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18230.8340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18230.8340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22498.0742, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22792.1816, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22792.1816, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20940.7109, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20907.1914, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20907.1914, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13448.7402, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13277.4219, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13277.4219, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4687.3379, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4686.8936, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4686.8936, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(111.6572, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(158.0676, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(158.0676, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1343.6819, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1734.2300, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1734.2300, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8386.9717, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8427.7676, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8427.7676, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16713.6660, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17047.6035, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17047.6035, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22894.6973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22362.9648, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22362.9648, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22377.0508, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22332.7344, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22332.7344, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16082.1436, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16605.6719, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16605.6719, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7413.9873, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7692.9629, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7692.9629, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(1052.5076, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(869.6763, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(869.6763, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(680.5939, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(702.9707, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(702.9707, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6345.9126, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6731.3340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6731.3340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15273.6475, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14632.1709, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14632.1709, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21531.7480, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21456.3926, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21456.3926, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23319.0859, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23039.9766, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23039.9766, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18405.6816, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18682.4629, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18682.4629, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9842.8330, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10057.3135, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10057.3135, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2268.4709, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2197.3306, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2197.3306, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.4422, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(90.8576, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.8576, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4289.4731, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4581.5352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4581.5352, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12664.6182, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12279.3359, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12279.3359, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20000.9863, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19808.3633, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19808.3633, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23413.5273, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22121.1094, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22121.1094, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19455.8320, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19912.5820, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19912.5820, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12122.1104, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11716.9980, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11716.9980, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3599.4827, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3967.2356, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3967.2356, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11.9336, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15.2872, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15.2872, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2120.8301, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2338.3201, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2338.3201, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9360.0781, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9187.5479, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9187.5479, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17221.0527, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16564.7891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16564.7891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21673.1895, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21921.8711, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21921.8711, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21333.7383, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22071.0234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22071.0234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15379.4482, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15616.0840, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15616.0840, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6495.2627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6418.6011, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6418.6011, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(530.1203, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(450.5763, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(450.5763, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1111.5872, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1180.8245, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1180.8245, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7350.6304, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6860.1582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6860.1582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15019.7939, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15539.1602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15539.1602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22030.5449, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21585.0664, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21585.0664, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22694.3789, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22739.3770, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22739.3770, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17414.7324, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17903.1387, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17903.1387, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8832.5264, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8647.0088, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8647.0088, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1520.4244, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1536.0518, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1536.0518, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(237.1927, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(272.2152, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(272.2152, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5179.5947, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4929.4668, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4929.4668, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12925.0996, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13753.8750, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13753.8750, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21301.6699, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20718.1719, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20718.1719, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23510.9355, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23543.3848, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23543.3848, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19746.1367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18521.4180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18521.4180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10299.7568, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10957.4980, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10957.4980, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2928.5747, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2874.1064, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2874.1064, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.7997, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(64.5378, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.5378, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3184.5627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2913.2773, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2913.2773, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10171.8809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10746.6797, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10746.6797, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18756.7695, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18076.1211, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18076.1211, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22525.7500, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21857.7031, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21857.7031, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(20275.4531, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22199.5176, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22199.5176, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14668.6934, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14306.0098, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14306.0098, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5303.8799, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5193.4492, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5193.4492, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(193.5675, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(258.2149, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(258.2149, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1498.3866, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1692.3738, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1692.3738, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8430.9453, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8210.3271, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8210.3271, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16557.5215, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15722.9873, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15722.9873, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21647.2793, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22166.6582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22166.6582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22655.8086, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22953.5977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22953.5977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17027.5078, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16682.1367, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16682.1367, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7692.9697, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7640.4316, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7640.4316, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1092.8861, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1137.4395, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1137.4395, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(464.3595, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(597.5461, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(597.5461, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5946.9004, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5324.9736, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5324.9736, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13261.2100, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12940.3125, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12940.3125, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19952.9023, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20224.6191, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20224.6191, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22769.5586, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22776.0957, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22776.0957, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18931.6309, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19429.6172, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19429.6172, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10889.4414, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11467.1318, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11467.1318, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3140.3394, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2762.5454, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2762.5454, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.5241, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(31.5565, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.5565, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3315.9622, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3568.9670, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3568.9670, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11319.5996, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10990.4512, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10990.4512, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18953.5898, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18975.4570, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18975.4570, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23334.1680, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23821.0254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23821.0254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21823.7227, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21609.3594, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21609.3594, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13848.6035, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13963.7061, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13963.7061, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4966.5972, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4653.6943, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4653.6943, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(104.1464, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(177.8198, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(177.8198, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1320.6483, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1816.0902, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1816.0902, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8649.3193, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8729.5479, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8729.5479, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17246.5098, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16504.3184, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16504.3184, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22460.6953, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23156.3262, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23156.3262, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23449.2031, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23125.6582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23125.6582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16968.4297, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16616.8340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16616.8340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7530.9121, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7270.1582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7270.1582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(883.8329, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1242.6647, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1242.6647, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(400.2947, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(389.7681, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(389.7681, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5375.8770, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5889.2046, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5889.2046, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14283.7285, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13560.7080, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13560.7080, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20775.5820, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21071.7969, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21071.7969, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23704.3750, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24166.9766, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24166.9766, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20173.6387, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19753.9062, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19753.9062, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11076.8770, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11194.1836, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11194.1836, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2968.3003, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2894.6172, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2894.6172, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(57.8257, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(20.8603, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.8603, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3498.4819, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2954.4856, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2954.4856, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10266.7930, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10782.8662, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10782.8662, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18880.5840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18791.8457, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18791.8457, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23456.8477, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24393.7441, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24393.7441, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22777.4316, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22215.5957, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22215.5957, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14661.0020, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14690.5215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14690.5215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5566.5635, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5391.6665, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5391.6665, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(240.7010, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(225.5428, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(225.5428, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1470.5144, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1464.1526, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1464.1526, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7928.3965, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7749.3994, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7749.3994, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16080.0420, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16273.3984, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16273.3984, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22624.2090, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22615.3633, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22615.3633, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23486.0078, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23627.5293, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23627.5293, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17983.5449, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17857.8633, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17857.8633, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8709.5723, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8382.1895, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8382.1895, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1429.6307, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1430.1769, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1430.1769, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(311.3996, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(275.5343, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(275.5343, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5004.6348, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4813.2881, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4813.2881, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12758.8037, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12716.2676, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12716.2676, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20197.2344, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19381.3809, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19381.3809, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22630.1973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23479.9180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23479.9180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20456.8809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21260.0215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21260.0215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12898.2754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12763.3818, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12763.3818, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4042.4583, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3532.9958, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3532.9958, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(90.8124, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.9001, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.9001, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2470.9697, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2483.5471, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2483.5471, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9617.9131, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9127.0625, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9127.0625, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17184.7227, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16616.3516, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16616.3516, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21929.1152, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22338.6660, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22338.6660, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22068.4121, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23355.5957, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23355.5957, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16767.1191, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16405.0215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16405.0215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7189.9434, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7297.4243, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7297.4243, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(878.2931, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(717.4229, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(717.4229, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(871.5898, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(686.2963, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(686.2963, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6211.3975, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6082.7734, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6082.7734, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14282.7559, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14423.6396, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14423.6396, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21480.4375, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21964.2188, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21964.2188, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24159.2168, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23082.4160, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23082.4160, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18775.5195, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19515.0801, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19515.0801, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10710.9648, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11019.3975, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11019.3975, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2844.2388, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2593.3494, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2593.3494, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(27.2920, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(31.1669, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(31.1669, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3717.2737, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3747.4678, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3747.4678, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11542.3564, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11587.1631, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11587.1631, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(19635.0059, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19217.2305, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19217.2305, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23514.3750, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23938.0605, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23938.0605, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21947.3086, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21719.3535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21719.3535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14028.3584, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13639.3604, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13639.3604, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4881.9604, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5139.9907, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5139.9907, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(187.1116, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(168.8155, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(168.8155, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1655.1862, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1486.4547, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1486.4547, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7905.4146, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8773.1670, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8773.1670, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17339.3047, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16669.5215, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16669.5215, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22747.6445, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23101.3145, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23101.3145, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23572.7012, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22951.2734, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22951.2734, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17057.8105, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16838.6660, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16838.6660, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7869.0107, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7552.6284, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7552.6284, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1052.7314, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1160.3944, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1160.3944, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(370.1369, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(366.1548, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(366.1548, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5319.4995, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5275.9033, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5275.9033, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13276.3740, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12767.8857, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12767.8857, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19917.6816, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20082.2051, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20082.2051, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22955.5918, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22218.4199, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22218.4199, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18888.1211, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19833.2363, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19833.2363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11611.2578, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11748.3604, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11748.3604, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3451.5979, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3128.1521, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3128.1521, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(29.7851, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(59.2406, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.2406, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2700.1562, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2493.1001, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2493.1001, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9515.9727, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10196.6934, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10196.6934, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18355.9355, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17577.0664, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17577.0664, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22484.2461, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22665.7070, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22665.7070, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21650.3145, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21760.6953, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21760.6953, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14817.8877, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14696.5605, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14696.5605, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5838.7148, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6568.1768, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6568.1768, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(642.3441, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(529.5379, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(529.5379, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1055.5537, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1104.0988, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1104.0988, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7199.3789, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7641.8813, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7641.8813, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16200.3662, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16308.1016, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16308.1016, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22899.5840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21948.5449, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21948.5449, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23002.0234, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23415.4180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23415.4180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17974.4453, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18362.8906, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18362.8906, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9116.7578, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8420.6602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8420.6602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1444.2588, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1521.7468, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1521.7468, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(253.8080, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(124.7844, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(124.7844, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4580.6099, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5214.6021, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5214.6021, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13455.2148, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12951.1602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12951.1602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20467.2500, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19561.2324, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19561.2324, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22720.5469, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24045.9629, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24045.9629, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20794.9453, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20777.3398, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20777.3398, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12338.7969, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12296.8467, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12296.8467, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3713.5542, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(3687.4636, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3687.4636, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(38.6044, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(163.7390, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(163.7390, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3120.5161, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2563.5850, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2563.5850, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9696.5840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9886.0254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9886.0254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18092.1895, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18911.2285, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18911.2285, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24143.9922, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23734.0879, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23734.0879, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22708.5625, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22431.3867, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22431.3867, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15324.6074, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15711.4688, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15711.4688, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6408.9746, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6073.8213, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6073.8213, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(447.8278, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(413.5687, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(413.5687, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1218.0195, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1217.6929, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1217.6929, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7413.3057, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6838.1426, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6838.1426, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15024.2334, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15342.9434, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15342.9434, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21974.8730, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21741.0078, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21741.0078, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23198.5293, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23284.5156, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23284.5156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18300.7129, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18224.8066, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18224.8066, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9357.5547, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9190.7012, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9190.7012, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1849.5746, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2316.8408, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2316.8408, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(42.6614, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(56.9986, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.9986, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4190.2300, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3833.9148, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3833.9148, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11470.9268, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11634.3311, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11634.3311, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19466.3516, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19532.7891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19532.7891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23600.9844, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24751.8770, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24751.8770, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22376.6973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22021.7051, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22021.7051, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13926.2158, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13694.1064, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13694.1064, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4722.9194, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4844.9990, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4844.9990, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(99.6907, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(84.3575, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(84.3575, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1982.9514, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1673.0977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1673.0977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8317.9307, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8747.9609, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8747.9609, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17265.9043, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16419.3711, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16419.3711, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22450.0586, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23738.7734, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23738.7734, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24215.7988, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22506.9688, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22506.9688, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16670.1445, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18678.4004, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18678.4004, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9125.7627, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8519.4473, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8519.4473, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1406.1956, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1598.5020, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1598.5020, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(268.4055, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(292.1537, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(292.1537, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5167.2085, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5510.3091, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5510.3091, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13981.3779, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14004.8936, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14004.8936, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21791.1699, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22005.0195, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22005.0195, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25222.4629, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24767.9980, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24767.9980, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21264.6230, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20640.2422, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20640.2422, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12169.1074, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13022.1152, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13022.1152, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4086.8740, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4044.7998, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4044.7998, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(60.5647, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(32.4166, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.4166, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2783.4468, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2861.0879, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2861.0879, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10364.5303, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9818.3154, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9818.3154, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18173.5918, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(17874.0352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17874.0352, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23378.1523, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23723.0078, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23723.0078, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23397.4844, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23343.6914, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23343.6914, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16770.1855, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17060.8672, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17060.8672, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7693.7739, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7604.2515, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7604.2515, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1018.4114, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1030.4460, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1030.4460, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(529.9774, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(553.6343, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(553.6343, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5937.0469, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5550.4585, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5550.4585, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13700.5293, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14454.5928, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14454.5928, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22017.5957, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21492.6758, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21492.6758, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24380.3711, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24197.4043, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24197.4043, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20604.1465, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20368.4336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20368.4336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11950.0049, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12826.4609, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12826.4609, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4010.1675, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3950.3618, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3950.3618, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(46.1420, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.0912, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.0912, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2384.3564, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1623.7888, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1623.7888, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8239.7930, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9427.3447, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9427.3447, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18057.1387, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18981.8711, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18981.8711, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25105.8672, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25559.5801, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25559.5801, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25560.0684, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24586.1621, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24586.1621, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17981.5078, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18414.2480, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18414.2480, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8634.3379, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8466.9316, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8466.9316, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1326.1813, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1276.3406, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1276.3406, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(385.3828, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(352.0684, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(352.0684, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5378.8491, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5542.7476, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5542.7476, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14031.6270, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14126.2812, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14126.2812, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22080.5938, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21745.0469, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21745.0469, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25296.1875, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24818.3809, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24818.3809, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21876.4277, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23585.1113, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23585.1113, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14876.6201, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14009.9785, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14009.9785, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4809.7417, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4793.2646, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4793.2646, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(95.9172, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(112.3592, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(112.3592, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1698.4238, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2143.5491, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2143.5491, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9292.2188, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8536.9004, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8536.9004, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17117.4277, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17541.3926, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17541.3926, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24109.2852, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24000.6211, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24000.6211, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25063.8398, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24822.5137, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24822.5137, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19351.6309, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19975.9453, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19975.9453, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10491.3594, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10375.9824, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10375.9824, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2379.3572, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2359.7551, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2359.7551, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19.4759, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.5217, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.5217, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4025.3538, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3815.6238, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3815.6238, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11660.2363, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12626., grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12626., grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21181.9648, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21371.6016, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21371.6016, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(26220.2227, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25706.1328, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25706.1328, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24078.8926, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23295.5820, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23295.5820, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(15784.1113, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15563.3838, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15563.3838, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6344.4312, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6506.6299, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6506.6299, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(613.0547, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(595.8535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(595.8535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(929.2587, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(689.9109, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(689.9109, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6236.5278, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6516.3242, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6516.3242, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14956.5684, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15382.3418, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15382.3418, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22829.7051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22398.8848, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22398.8848, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25090.8418, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25660.0586, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25660.0586, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21786.2031, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21817.6699, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21817.6699, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12950.2090, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12201.6074, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12201.6074, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3651.5078, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3736.0352, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3736.0352, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(53.9826, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(50.8030, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.8030, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2151.9431, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2359.9504, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2359.9504, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9491.6152, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9336.9561, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9336.9561, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17828.9609, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18072.0547, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18072.0547, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24201.6172, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24346.2012, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24346.2012, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24834.2578, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24915.3867, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24915.3867, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18917.0645, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18560.4727, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18560.4727, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9209.7490, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9109.7109, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9109.7109, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1768.6223, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1904.3973, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1904.3973, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(86.8553, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(35.2090, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.2090, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4170.0381, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4264.6987, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4264.6987, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12257.8809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12381.5029, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12381.5029, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20564.0566, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20208.1250, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20208.1250, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24718.1094, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24834.9277, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24834.9277, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23167.1387, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22777.9883, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22777.9883, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15355.6602, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15541.9160, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15541.9160, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6322.9155, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6766.8389, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6766.8389, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(693.5618, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(797.3358, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(797.3358, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(754.0846, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(853.9270, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(853.9270, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6688.3892, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6691.0112, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6691.0112, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15238.2432, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15024.4648, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15024.4648, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22414.9141, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22861.6934, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22861.6934, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25604.3809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25333.4648, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25333.4648, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21483.5840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21286.4688, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21286.4688, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12576.2139, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13049.1553, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13049.1553, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4131.5010, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3816.2102, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3816.2102, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17.8635, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21.2931, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.2931, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2366.2012, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2042.6860, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2042.6860, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8934.7695, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9798.6611, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9798.6611, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18514.5645, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19062.7168, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19062.7168, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25291.5625, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24263.0605, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24263.0605, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24635., grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25224.2793, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25224.2793, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19092.8379, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18015.2676, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18015.2676, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8767.8301, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8898.6660, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8898.6660, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1654.2443, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1824.8630, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1824.8630, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(129.1431, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(135.3759, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(135.3759, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4519.6621, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4490.6938, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4490.6938, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12512.8252, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12332.2051, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12332.2051, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20346.4805, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20823.8906, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20823.8906, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25206.0996, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24854.4883, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24854.4883, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22950.5430, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23007.2695, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23007.2695, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15354.3604, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15166.9473, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15166.9473, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6003.0923, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6297.7393, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6297.7393, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(516.4844, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(462.9928, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(462.9928, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1097.4377, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1015.3116, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1015.3116, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6982.8301, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7183.2983, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7183.2983, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15724.1738, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14905.3359, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14905.3359, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21953.4375, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22436.6543, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22436.6543, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24818.0742, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24379.6992, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24379.6992, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20348.6172, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20632.5176, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20632.5176, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11941.4170, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11215.1152, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11215.1152, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3103.2466, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3717.8972, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3717.8972, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(33.0847, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(30.0392, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.0392, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2427.4326, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2426.7944, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2426.7944, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9507.5039, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9941.9707, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9941.9707, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18457.1680, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20010.6133, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20010.6133, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25947.1270, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23618.1582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23618.1582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23397.1211, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23781.6172, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23781.6172, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17374.0039, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17093.3418, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17093.3418, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7918.1558, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8062.5737, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8062.5737, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1255.8715, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1191.2102, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1191.2102, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(387.9902, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(500.4865, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(500.4865, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5677.1768, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5417.2979, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5417.2979, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13561.0625, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13499.8652, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13499.8652, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21053.9512, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20531.3730, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20531.3730, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23859.8574, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23899.6133, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23899.6133, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21007.8203, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22507., grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22507., grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14132.5938, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14144.3945, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14144.3945, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4976.6738, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4609.4658, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4609.4658, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(82.9863, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.3179, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.3179, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1908.1292, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2013.5996, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2013.5996, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8962.9131, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8882.6846, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8882.6846, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17447.6074, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17765.0234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17765.0234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24106.5254, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23774.0488, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23774.0488, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24479.7422, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25168.8574, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25168.8574, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19270.6035, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18277.4941, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18277.4941, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9046.6338, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9345.6992, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9345.6992, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1861.7083, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1662.4950, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1662.4950, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(233.4432, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7.9564, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.9564, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4046.3848, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4405.9678, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4405.9678, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(12425.6562, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11748.2617, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11748.2617, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19663.7188, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20734.7617, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20734.7617, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25134.4180, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25155.1309, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25155.1309, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23164.9590, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22804.6660, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22804.6660, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15047.3633, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15225.5771, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15225.5771, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5931.3447, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6356.7095, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6356.7095, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(540.5709, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(246.0118, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(246.0118, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1352.2734, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1449.2310, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1449.2310, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8059.8398, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7603.4443, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7603.4443, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16097.9932, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15145.5693, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15145.5693, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21838.1465, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22744.0332, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22744.0332, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24624.2109, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24898.5156, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24898.5156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20229.8281, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19764.2520, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19764.2520, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10851.6592, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10775.5303, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10775.5303, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2762.8027, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3120.8708, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3120.8708, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(118.4245, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(57.9895, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.9895, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3258.6675, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3164.4404, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3164.4404, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10686.4570, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10362.9863, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10362.9863, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18579.3242, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18460.1719, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18460.1719, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23694.5293, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24971.3691, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24971.3691, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24283.3906, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24227.9766, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24227.9766, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17221.9902, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16150.0283, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16150.0283, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6953.5366, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7112.3896, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7112.3896, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(821.5058, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1041.2747, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1041.2747, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(515.9740, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(583.5164, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(583.5164, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6019.2905, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5439.4404, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5439.4404, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13508.3633, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14150.5703, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14150.5703, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21741.6660, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21466.1641, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21466.1641, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24611.2812, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24772.5137, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24772.5137, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21469.1230, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22419.2891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22419.2891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13718.5898, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13575.9336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13575.9336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4470.1143, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4080.3567, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4080.3567, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4.1102, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.0014, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.0014, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2208.5659, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2296.4446, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2296.4446, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9430.5508, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9410.1250, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9410.1250, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17983.5312, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18261.3574, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18261.3574, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24384.3906, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24204.5645, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24204.5645, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24567.7949, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24547.8594, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24547.8594, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18450.2617, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17693.4629, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17693.4629, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8505.7324, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9009.8740, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9009.8740, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1699.1995, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1576.0051, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1576.0051, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(202.5944, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(175.8410, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(175.8410, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4627.8066, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5023.2949, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5023.2949, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13309.6113, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13399.6143, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13399.6143, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21398.0547, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20733.1699, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20733.1699, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24571.3320, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(24929.1680, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24929.1680, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22412.7559, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22354.5977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22354.5977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14301.9170, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13385.2891, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13385.2891, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4710.6694, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5411.7471, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5411.7471, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(234.7338, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(179.1242, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(179.1242, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1640.0271, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1569.2778, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1569.2778, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8067.5483, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7870.9839, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7870.9839, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16261.0293, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16530.8223, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16530.8223, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23112.5234, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23375.3027, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23375.3027, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24675.3359, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24145.1328, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24145.1328, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18985.2441, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19783.4883, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19783.4883, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10506.4775, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10507.7686, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10507.7686, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2495.9253, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2104.0996, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2104.0996, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24.7946, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22.6525, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.6525, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4015.9138, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4078.2505, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4078.2505, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12016.9629, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11497.0801, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11497.0801, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19518.4453, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19966.4902, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19966.4902, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24574.2734, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24612.5977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24612.5977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22999.1836, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22502.2910, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22502.2910, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15120.1504, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15523.2158, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15523.2158, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6280.3662, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6227.1875, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6227.1875, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(509.8361, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(500.4180, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(500.4180, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1035.9333, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(857.6374, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(857.6374, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6638.2349, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6904.2725, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6904.2725, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15360.5527, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15284.6582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15284.6582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22385.6055, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22504.2832, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22504.2832, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24727.9258, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25706.4902, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25706.4902, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21242.6699, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21752.5098, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21752.5098, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12398.9277, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11439.3838, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11439.3838, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3040.1650, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3201.6726, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3201.6726, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(41.5271, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(64.0725, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(64.0725, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3304.8765, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3355.7329, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3355.7329, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11131.2402, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11296.6406, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11296.6406, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19843.1641, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18988.7773, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18988.7773, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24203.0898, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24805.6621, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24805.6621, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24033.1562, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24136.1777, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24136.1777, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17103.4805, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17179.5156, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17179.5156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7604.0142, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7410.8174, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7410.8174, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(921.8783, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(911.4894, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(911.4894, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(621.8722, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(676.0995, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(676.0995, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6236.4292, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6123.0679, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6123.0679, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14552.8154, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14278.0732, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14278.0732, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21788.1855, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22319.9668, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22319.9668, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25382.5312, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25284.6270, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25284.6270, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21766.0879, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21281.8047, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21281.8047, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12768.7754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13270.2578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13270.2578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4350.8438, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3938.5552, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3938.5552, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(45.9306, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(25.3825, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25.3825, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2242.6528, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2160.9797, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2160.9797, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9109.0098, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8725.2012, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8725.2012, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17040.6660, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17788.1641, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17788.1641, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23986.4121, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23845.2578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23845.2578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24430.0605, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24238.6543, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24238.6543, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18429.5000, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19383.7930, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19383.7930, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9795.9326, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9433.0762, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9433.0762, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1859.8248, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2016.2013, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2016.2013, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5.8947, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(87.1108, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.1108, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4479.5684, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4644.3057, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4644.3057, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12891.8877, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12673.4785, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12673.4785, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20856.2051, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21879.1230, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21879.1230, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(26296.2344, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25895.2969, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25895.2969, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23717.9688, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24030.6133, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24030.6133, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15879.9463, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15880.1074, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15880.1074, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6219.7979, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6132.2295, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6132.2295, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(434.9888, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(454.1739, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(454.1739, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1197.1345, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1113.1965, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1113.1965, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7259.0708, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7720.9619, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7720.9619, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16607.7285, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15834.7266, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15834.7266, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23086.2383, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24330.2656, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24330.2656, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(26713.0078, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25532.8340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25532.8340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21203.7246, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21210.3047, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21210.3047, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12223.9209, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12537.9707, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12537.9707, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3765.7524, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3997.1699, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3997.1699, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(32.6537, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19.6122, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.6122, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2716.6343, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2184.4814, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2184.4814, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9209.1016, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9718.7383, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9718.7383, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18427.8633, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18102.7617, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18102.7617, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24345.3809, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24586.8711, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24586.8711, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25266.6836, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23808.3398, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23808.3398, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18226.0449, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18978.1602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18978.1602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9757.8926, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10485.5400, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10485.5400, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2480.4902, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2164.9468, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2164.9468, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.3430, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2.5416, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.5416, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3964.3362, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4320.0327, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4320.0327, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12493.2246, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11597.0312, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11597.0312, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19722.0840, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20060.0273, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20060.0273, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24897.6895, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24206.5938, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24206.5938, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23020.5137, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23556.4590, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23556.4590, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16445.5723, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16314.0771, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16314.0771, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7053.4170, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7002.8384, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7002.8384, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(815.4976, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(753.5948, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(753.5948, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(751.3839, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(561.5734, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(561.5734, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5856.5234, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(6310.1538, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6310.1538, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14722.1162, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12788.2793, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12788.2793, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19966.7480, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21547.2266, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21547.2266, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24796.8965, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24276.0117, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24276.0117, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21216.7969, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22254.8848, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22254.8848, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13881.5869, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13179.8789, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13179.8789, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4424.7476, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4516.2339, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4516.2339, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(48.0999, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(179.5392, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(179.5392, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1632.3137, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1848.2819, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1848.2819, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8647.5332, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8246.8672, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8246.8672, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16652.8750, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16619.7266, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16619.7266, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23080.1641, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23257.6582, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23257.6582, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24489.5273, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24452.0156, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24452.0156, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19262.8477, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19411.0586, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19411.0586, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10271.5762, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10376.3164, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10376.3164, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2452.5442, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2597., grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2597., grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(85.3842, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(57.8090, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.8090, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3580.8315, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3743.3628, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3743.3628, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11595.8975, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11940.4707, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11940.4707, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20346.2930, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20219.8320, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20219.8320, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25129.2734, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24910.8867, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24910.8867, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23604.9766, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23067.7148, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23067.7148, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15847.5801, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17265.1562, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17265.1562, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7530.6235, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7031.9150, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7031.9150, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(767.1033, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(685.3080, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(685.3080, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(813.6995, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(863.7943, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(863.7943, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6771.6294, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6398.3701, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6398.3701, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14814.0361, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15048.0820, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15048.0820, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22501.8164, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22425.0703, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22425.0703, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25206.7598, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24974.1367, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24974.1367, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21251.9883, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20875.2969, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20875.2969, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12362.3506, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12949.9912, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12949.9912, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4141.9097, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4035.9124, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4035.9124, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3.3345, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4.7220, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.7220, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1891.4127, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2276.9846, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2276.9846, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9410.4463, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9297.5684, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9297.5684, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17888.4648, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17953.8965, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17953.8965, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24175.9238, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23888.7188, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23888.7188, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24503.8516, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25792.3535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25792.3535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(19779.9668, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18658.9941, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18658.9941, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9276.8105, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9454.9434, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9454.9434, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1900.6870, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1971.5535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1971.5535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(100.5710, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(118.9067, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(118.9067, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4568.2681, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4697.5654, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4697.5654, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12933.2988, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12168.7432, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12168.7432, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20162.5820, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(20146.1328, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20146.1328, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24541.6602, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24912.1641, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24912.1641, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23139.1387, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23919.5801, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23919.5801, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16146.3105, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(14736.0293, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(14736.0293, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5749.1792, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6218.5518, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6218.5518, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(516.4135, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(538.1691, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(538.1691, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(947.2504, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1065.7931, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1065.7931, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7085.6006, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7121.8623, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7121.8623, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15633.1621, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15484.5684, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15484.5684, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22613.7461, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22223.2480, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22223.2480, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24516.8027, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25983.5488, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25983.5488, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21675.7637, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20778.6680, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20778.6680, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11869.4531, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(11857.0586, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(11857.0586, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3391.5132, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3018.2126, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3018.2126, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(97.2141, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(69.9615, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(69.9615, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2875.3831, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2916.4102, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2916.4102, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10309.6377, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10028.8418, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10028.8418, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18297.1914, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18995.0879, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18995.0879, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24553.7363, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23303.5234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23303.5234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22993.1582, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24508.5898, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24508.5898, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(17895.7461, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17230.1094, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17230.1094, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7915.8193, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8179.6948, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8179.6948, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1260.1953, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1082.3359, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1082.3359, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(509.8756, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(345.9073, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(345.9073, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5304.0801, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5679.9009, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5679.9009, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14036.9043, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13956.3799, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13956.3799, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21653.1973, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21610.1152, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21610.1152, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24974.0059, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24296.2031, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24296.2031, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21267.6113, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22790.3555, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22790.3555, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14260.5400, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13205.9512, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13205.9512, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4388.6748, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4678.8301, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4678.8301, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(101.9474, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(103.9218, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(103.9218, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1873.9292, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1907.7285, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1907.7285, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8731.7520, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8506.9648, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8506.9648, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16979.1621, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17850.5742, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17850.5742, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24351.8184, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24159.2461, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24159.2461, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25020.5449, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24587.7109, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24587.7109, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18935.0410, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18837.4297, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18837.4297, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9570.5420, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10143.1152, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10143.1152, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2227.7490, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2173.5161, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2173.5161, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9.1578, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(132.4538, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(132.4538, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(4514.5640, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4144.7769, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4144.7769, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12108.5254, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12342.7461, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12342.7461, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20569.5703, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21211.2129, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21211.2129, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25790.4863, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24329.9531, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24329.9531, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22561.9785, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(23583.4727, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23583.4727, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15897.0137, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15858.7188, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15858.7188, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(6417.1348, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(6366.5483, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6366.5483, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(537.1934, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(484.9053, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(484.9053, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1051.1650, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(971.9641, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(971.9641, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(6895.1709, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7315.9624, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7315.9624, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(16014.8369, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15552.1367, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15552.1367, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(22746.6328, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22655.9746, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22655.9746, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24997.6367, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25350.5000, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25350.5000, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21126.2773, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20909.2520, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20909.2520, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12018.9375, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12737.4238, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12737.4238, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3832.0715, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3531.7266, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3531.7266, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(59.1260, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(72.9121, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.9121, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2606.5127, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2872.4856, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2872.4856, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10411.1885, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(10306.6426, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(10306.6426, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18929.7676, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18535.8867, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18535.8867, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24353.6250, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24967.5605, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24967.5605, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24987.7676, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25026.2949, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25026.2949, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18565.2754, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18594.9746, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18594.9746, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(8951.9180, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(8472.7363, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8472.7363, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1392.2632, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1652.6748, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1652.6748, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(221.1317, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(265.3077, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(265.3077, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5115.3047, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5169.6855, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5169.6855, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(13520.7520, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(13036.5439, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13036.5439, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(20987.2539, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(21846.3496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21846.3496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25936.3848, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25534.2383, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25534.2383, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23126.7383, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(22753.5234, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22753.5234, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(14771.7988, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15473.0879, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15473.0879, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(5978.3350, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5575.1978, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5575.1978, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(300.6893, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(392.6061, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(392.6061, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1005.7050, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1433.4543, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1433.4543, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(7958.3442, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(7052.5327, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7052.5327, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(15496.8262, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(15879.1416, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(15879.1416, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(23038.5039, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24042.6797, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24042.6797, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(26285.5664, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25493.2422, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25493.2422, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(21062.3750, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(20516.4688, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20516.4688, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(11623.7236, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(12837.3535, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(12837.3535, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3917.0393, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(3618.4282, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3618.4282, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(58.0138, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(50.9698, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.9698, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2649.1123, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(2786.4263, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2786.4263, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(10295.3311, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9744.3340, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9744.3340, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18255.5723, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(18236.6602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18236.6602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(24253.5312, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(25083.5098, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(25083.5098, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(25460.6504, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(24896.5293, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24896.5293, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(18822.3203, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(19030.0840, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19030.0840, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(9509.5430, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9054.3525, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9054.3525, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1744.7269, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(1882.1013, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1882.1013, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(111.9008, grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(72.5284, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.5284, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3781.4727, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(4054.5830, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4054.5830, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(12035.8457, grad_fn=<CopyBackwards>)\n",
      "Loss for b is tensor(13.9404, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(4820.3184, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(743.0152, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(653.1400, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss for b is tensor(3202.8928, grad_fn=<NormBackward1>) at iteration 0\n",
      "1\n",
      "RSS at n=1 tensor(160.4064)\n",
      "It took: 288.2289836 seconds to finish running\n",
      "The best RSS value was tensor(160.3343)\n",
      "[tensor(160.3343), tensor(160.4064)]\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "RSS_values = list()\n",
    "A = np.zeros((K,N))#.tolist()\n",
    "B = np.zeros((N,K))#.tolist()\n",
    "\n",
    "# LOOP UNTIL RSS IS LOW\n",
    "for n in range(2): #N\n",
    "    Q = Z.T @ Z\n",
    "    Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "    # LOOP THROUGH ENTIRE A\n",
    "    for i in range(N):\n",
    "        q = Z.T @ X[:,i]\n",
    "        qt = torch.tensor(q,requires_grad=False).float()\n",
    "        \n",
    "        if n == 0:\n",
    "            a_i = torch.autograd.Variable(torch.rand(K, 1), requires_grad=True) # eller er det Kx1 ?\n",
    "            optimizer_a = optim.Adam([a_i], lr=0.01)\n",
    "        \n",
    "        stop_loss = 1e-5\n",
    "        step_size = 0.001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(a_i,Qt,qt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(100): # 100000\n",
    "            #optimizer_a.zero_grad()\n",
    "            Delta = error(a_i,Qt,qt) \n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            optimizer_a.step()\n",
    "            #a_i.data -= step_size * a_i.grad.data # step\n",
    "            #a_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss for a is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "        \n",
    "        A[:,i] = np.array(a_i.tolist()).flatten() \n",
    "        \n",
    "        print('Loss after: %s' % (torch.norm( error(a_i,Qt,qt) )))\n",
    "    \n",
    "    A = applyConstrains(A)\n",
    "    Z = X @ A.T @ np.linalg.inv(A@A.T)\n",
    "    \n",
    "    \n",
    "    # LOOP THROUGH ENTIRE B\n",
    "    for i in range(K): #K\n",
    "        r = X.T @ Z[:,i]\n",
    "        rt = torch.tensor(r,requires_grad=False).float()\n",
    "        \n",
    "        if n == 0:\n",
    "            b_i = torch.autograd.Variable(torch.randn(N,1), requires_grad=True)\n",
    "            optimizer_b = optim.Adam([b_i], lr=0.01)\n",
    "        \n",
    "        \n",
    "        stop_loss = 1e-5\n",
    "        step_size = 0.001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(b_i,Rt,rt)\n",
    "        #print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(10000): # 100000\n",
    "            optimizer_b.zero_grad()\n",
    "            Delta = error(b_i,Rt,rt)\n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            optimizer_b.step()\n",
    "            \n",
    "            # b_i.data -= step_size * b_i.grad.data # step\n",
    "            # b_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss for b is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "\n",
    "        B[:,i] = np.array(b_i.tolist()).flatten() \n",
    "        #print('Loss after: %s' % (torch.norm( error(b_i,Rt,rt) )))    \n",
    "    \n",
    "    # apply softmax here\n",
    "    B = applyConstrains(B)\n",
    "    Z = X @ B\n",
    "    \n",
    "    print(n)\n",
    "    Zt = torch.tensor(Z, requires_grad=False).float()\n",
    "    At = torch.tensor(A, requires_grad=False).float()\n",
    "    Xt = torch.tensor(X,requires_grad=False).float()\n",
    "    print(\"RSS at n=%s\" % n, torch.norm(Xt-Zt@At,p='fro'))\n",
    "    RSS_values.append( torch.norm(Xt-Zt@At,p='fro'))\n",
    "    \n",
    "end = timer()\n",
    "\n",
    "print(\"It took: {0} seconds to finish running\".format(end - start))\n",
    "print(\"The best RSS value was\", min(RSS_values))\n",
    "print(RSS_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adbd8a",
   "metadata": {},
   "source": [
    "## Time at datasize 1000 x 17 for:\n",
    "### Adam\n",
    "$\\textbf{Time}$ in seconds: 351.60987780000005 \n",
    "\n",
    "$\\textbf{RSS:}$ 160.3371 \n",
    "\n",
    "$\\textbf{Step size:}$ 0.00001\n",
    "\n",
    "$\\textbf{Stop loss:}$ $1\\cdot 10^{-6}$\n",
    "\n",
    "$\\textbf{Learning rate:}$ 0.05\n",
    "\n",
    "____________________________________________\n",
    "$\\textbf{Time}$ in seconds: 288.2289836 \n",
    "\n",
    "$\\textbf{RSS:}$ 160.4064\n",
    "\n",
    "$\\textbf{Step size:}$ 0.001\n",
    "\n",
    "$\\textbf{Stop loss:}$ $1\\cdot 10^{-5}$\n",
    "\n",
    "$\\textbf{Learning rate:}$ 0.01\n",
    "\n",
    "_____________________________________________\n",
    "\n",
    "### Ordinal GD(?)\n",
    "$\\textbf{Time}$ in seconds: 236.4697258000001\n",
    "\n",
    "$\\textbf{RSS:}$ 160.3474\n",
    "\n",
    "$\\textbf{Step size:}$ 0.00001\n",
    "\n",
    "$\\textbf{Stop loss:}$ $1\\cdot 10^{-6}$\n",
    "______________________________________________\n",
    "\n",
    "$\\textbf{Time}$ in seconds: 220.46896449999986\n",
    "\n",
    "$\\textbf{RSS:}$ 160.3430\n",
    "\n",
    "$\\textbf{Step size:}$ 0.001\n",
    "\n",
    "$\\textbf{Stop loss:}$ $1\\cdot 10^{-5}$\n",
    "\n",
    "____________________________________________\n",
    "### SGD\n",
    "$\\textbf{Time}$ in seconds: Never converged\n",
    "\n",
    "$\\textbf{RSS:}$ never converged $\\rightarrow$ loss after for $\\textbf{a}_i$ was consistently inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2cfbdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AA:\n",
    "    \"\"\"\n",
    "    Class for applying conventional archetypal analysis.\n",
    "    \n",
    "    Input: \n",
    "    X = M x N matrix of data (features x samples)\n",
    "    \n",
    "    K = number of archetypes to be computed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, K):\n",
    "        \n",
    "        self.M, self.N = X.shape\n",
    "        self.X = X\n",
    "        self.Rt = torch.tensor(X.T @ X,requires_grad=False).float()\n",
    "        self.K = K\n",
    "    \n",
    "    \n",
    "    def applyConstraints(self, A):\n",
    "        return softmax(A, axis = 0)\n",
    "    \n",
    "    def error(self, a_i, Qt, qt):\n",
    "        return (0.5 * a_i.T @ Qt @ a_i) - (qt.T @ a_i)\n",
    "        \n",
    "    \n",
    "    def furthestSum(self):\n",
    "        # Choose a random point for initialization\n",
    "        idx = int(np.random.choice(range(0,N)))\n",
    "        x_j = self.X[:,idx]\n",
    "\n",
    "        j_news = list()\n",
    "        j_news.append(idx)\n",
    "\n",
    "        excluded = [idx]\n",
    "\n",
    "        # Loop over the K archetypes\n",
    "        for n in range(self.K):\n",
    "            best_val = 0\n",
    "            best_idx = 0\n",
    "            # Loop over all unseen samples\n",
    "            for i in range(N-len(excluded)):\n",
    "                if i not in excluded:\n",
    "                    val = 0\n",
    "                    # sum over each element for each point\n",
    "                    for ele in j_news:\n",
    "                        for j in range(M):\n",
    "\n",
    "                            val += np.abs(self.X[j,i] - self.X[j , ele])\n",
    "                    if val > best_val:\n",
    "                        best_val = val\n",
    "                        best_idx = i\n",
    "\n",
    "            j_news.append(int(best_idx))\n",
    "            excluded.append(best_idx)\n",
    "            # Remove the random initialization\n",
    "            if n == 0:\n",
    "                j_news.pop(0)\n",
    "                excluded.pop(0)\n",
    "        return j_news\n",
    "\n",
    "    \"\"\"\n",
    "    Function for applying conventional archetypal analysis:\n",
    "    -----------------------------------------------------------------------\n",
    "        Inputs: \n",
    "            self\n",
    "            Z: Matrix that will contain the archetypes, initialize the values using furthestSum\n",
    "            time: Boolean, if True the time of convergence will be measured\n",
    "            lr: Learning rate for the optimizer\n",
    "            stop_loss: stop loss value for the inner loop\n",
    "            amsgrad: Boolean, if True uses the \"amsgrad\" method for Adam optimizer\n",
    "            n_outer: Number of iterations to run the outermost loop\n",
    "            n_inner: Number of iterations to run the inner loop\n",
    "    \"\"\"\n",
    "    def applyAA(self, Z, time = True, lr = 0.01, stop_loss=1e-05, amsgrad=True, n_outer=2, n_inner=1000):\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        if time == True:\n",
    "            start = timer()\n",
    "\n",
    "        RSS_values = list()\n",
    "        A = np.zeros((self.K, self.N))#.tolist()\n",
    "        B = np.zeros((self.N, self.K))#.tolist()\n",
    "\n",
    "        # LOOP UNTIL RSS IS LOW\n",
    "        for n in range(n_outer):\n",
    "            Q = Z.T @ Z\n",
    "            Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "            # LOOP THROUGH ENTIRE A\n",
    "            for i in range(self.N):\n",
    "                q = Z.T @ self.X[:,i]\n",
    "                qt = torch.tensor(q,requires_grad=False).float()\n",
    "\n",
    "                if n == 0:\n",
    "                    a_i = torch.autograd.Variable(torch.rand(self.K, 1), requires_grad=True) # eller er det Kx1 ?\n",
    "                    optimizer_a = optim.Adam([a_i], lr=0.01, amsgrad = amsgrad)\n",
    "\n",
    "                \n",
    "\n",
    "                err = error(a_i,Qt,qt)\n",
    "                print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "                # TRAINING LOOP\n",
    "                for k in range(n_inner): # 100000\n",
    "                    optimizer_a.zero_grad()\n",
    "                    Delta = error(a_i,Qt,qt) \n",
    "                    L = torch.norm(Delta, p=2)\n",
    "                    L.backward()\n",
    "                    optimizer_a.step()\n",
    "                    \n",
    "                    if k % 10000 == 0: print('Loss for a is %s at iteration %i' % (L, k))\n",
    "                    if abs(L) < stop_loss:\n",
    "                        print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                        break\n",
    "\n",
    "                A[:,i] = np.array(a_i.tolist()).flatten() \n",
    "\n",
    "                print('Loss after: %s' % (torch.norm( error(a_i,Qt,qt) )))\n",
    "\n",
    "            A = applyConstrains(A)\n",
    "            Z = self.X @ A.T @ np.linalg.inv(A@A.T)\n",
    "\n",
    "\n",
    "            # LOOP THROUGH ENTIRE B\n",
    "            for i in range(self.K): #K\n",
    "                r = self.X.T @ Z[:,i]\n",
    "                rt = torch.tensor(r,requires_grad=False).float()\n",
    "\n",
    "                if n == 0:\n",
    "                    b_i = torch.autograd.Variable(torch.randn(self.N,1), requires_grad=True)\n",
    "                    optimizer_b = optim.Adam([b_i], lr= lr, amsgrad = amsgrad)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                err = error(b_i, self.Rt, rt)\n",
    "                # print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "                # TRAINING LOOP\n",
    "                for k in range(n_inner):\n",
    "                    optimizer_b.zero_grad()\n",
    "                    Delta = error(b_i, self.Rt,rt)\n",
    "                    L = torch.norm(Delta, p=2)\n",
    "                    L.backward()\n",
    "                    optimizer_b.step()\n",
    "\n",
    "                    # b_i.data -= step_size * b_i.grad.data # step\n",
    "                    # b_i.grad.data.zero_()\n",
    "                    if k % 10000 == 0: print('Loss for b is %s at iteration %i' % (L, k))\n",
    "                    if abs(L) < stop_loss:\n",
    "                        print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                        break\n",
    "\n",
    "                B[:,i] = np.array(b_i.tolist()).flatten() \n",
    "                #print('Loss after: %s' % (torch.norm( error(b_i,Rt,rt) )))    \n",
    "\n",
    "            # apply softmax here\n",
    "            B = applyConstrains(B)\n",
    "            Z = self.X @ B\n",
    "\n",
    "            Zt = torch.tensor(Z, requires_grad=False).float()\n",
    "            At = torch.tensor(A, requires_grad=False).float()\n",
    "            Xt = torch.tensor(self.X,requires_grad=False).float()\n",
    "            print(\"RSS at n=%s\" % n, torch.norm(Xt-Zt@At, p='fro'))\n",
    "            RSS_values.append( torch.norm(Xt-Zt@At, p='fro'))\n",
    "        if time == True:\n",
    "            end = timer()\n",
    "            print(\"It took: {0} seconds to finish running\".format(end - start))\n",
    "            \n",
    "        print(\"The best RSS value was\", min(RSS_values))\n",
    "        \n",
    "        return RSS_values, Z, B, A\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bbb97",
   "metadata": {},
   "source": [
    "### Testing the class\n",
    "##### Defining variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06b0857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 1000)\n"
     ]
    }
   ],
   "source": [
    "X = df[['SD1', 'PO1', 'UN1', 'AC1', 'SC1',\n",
    "       'ST1', 'CO1', 'UN2', 'TR1', 'HD1', 'SD2','BE1','AC2', 'SC2', 'ST2',\n",
    "       'CO2', 'PO2', 'BE2', 'UN3', 'TR2','HD2']].iloc[range(100),:]\n",
    "\n",
    "X = X.to_numpy()\n",
    "X = X.T\n",
    "print(X.shape)\n",
    "K = 5\n",
    "\n",
    "lr = 0.01\n",
    "stop_loss = 1e-05\n",
    "armsgrad = True\n",
    "time = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0419dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before: tensor(789.0945, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(789.0945, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.3415, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(416.0961, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(416.0961, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.5127, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(694.3499, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(694.3499, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.6056, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(200.2817, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(200.2817, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.3930, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(148.3585, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(148.3585, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.8758, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(17.6309, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(17.6309, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.6827, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(538.1461, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(538.1461, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.0982, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(580.1072, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(580.1072, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.8638, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(684.3636, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(684.3636, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.6873, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(274.5775, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(274.5775, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.4017, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(57.1619, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.1619, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.1305, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(573.2366, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(573.2366, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.3571, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(315.3781, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(315.3781, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.4834, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(5.3409, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.3409, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.6619, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(148.9496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(148.9496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.2029, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(268.0377, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(268.0377, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.3193, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(175.1602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(175.1602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0171, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(36.4431, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.4431, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.2696, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(578.3174, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(578.3174, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.1745, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(65.9276, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.9276, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.0460, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(49.4328, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.4328, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.6375, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(82.8306, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.8306, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.4218, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(371.1699, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(371.1699, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.6158, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(45.2990, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.2990, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.7477, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(54.9836, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.9836, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.4204, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(236.6675, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.6675, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0853, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(686.9150, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(686.9150, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.7071, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(43.8681, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(43.8681, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.3860, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(221.3438, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(221.3438, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0794, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(806.3693, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(806.3693, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.4010, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(86.2949, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(86.2949, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(3.8434, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(207.2958, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(207.2958, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.8853, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(364.7152, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(364.7152, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.4256, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(260.3900, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(260.3900, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.9721, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(9.5020, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(9.5020, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0370, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(321.3445, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(321.3445, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(2.0733, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(300.0396, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(300.0396, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.5844, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(225.9975, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(225.9975, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.6795, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(615.4878, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(615.4878, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.3296, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(335.4227, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(335.4227, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.8971, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(121.9533, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(121.9533, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.4856, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(16.4429, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.4429, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.6780, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(146.6775, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(146.6775, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.8903, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(565.4977, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(565.4977, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.2548, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(61.4538, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.4538, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.8780, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(326.8290, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(326.8290, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.9090, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(424.8587, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(424.8587, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after: tensor(0.8954, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(106.7076, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(106.7076, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.5074, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(151.4555, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(151.4555, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.8972, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(81.0436, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.0436, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.0302, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(86.0173, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(86.0173, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.9207, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(128.8573, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(128.8573, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(0.8256, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(392.5201, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(392.5201, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss after: tensor(1.7514, grad_fn=<CopyBackwards>)\n",
      "Loss before: tensor(400.6244, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(400.6244, grad_fn=<NormBackward1>) at iteration 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28212/3944687190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfurthestSum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mAA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplyAA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_inner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28212/3437050119.py\u001b[0m in \u001b[0;36mapplyAA\u001b[1;34m(self, Z, time, lr, stop_loss, amsgrad, n_outer, n_inner)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0moptimizer_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[0mDelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m                     \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                     \u001b[0moptimizer_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m     \u001b[1;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AA_model = AA(X, K)\n",
    "Z = AA_model.X[:,AA_model.furthestSum()]\n",
    "\n",
    "RSS, Z, A, B = AA_model.applyAA(Z = Z, time = True, lr = 0.01, stop_loss=1e-05, amsgrad=True, n_outer=2, n_inner=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43cd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb31d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837aee97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc0d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef262b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
