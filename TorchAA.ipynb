{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "df0979c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from timeit import default_timer as timer\n",
    "from scipy.special import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "919a59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ESS8_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "17ef6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SD1', 'PO1', 'UN1', 'AC1', 'SC1',\n",
    "       'ST1', 'CO1', 'TR1', 'HD1', 'AC2', 'SC2', 'ST2',\n",
    "       'CO2', 'PO2', 'BE2', 'TR2', 'HD2']].iloc[range(100),:]\n",
    "X = X.to_numpy().T\n",
    "N, M = X.T.shape\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "716a419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "p = np.random.permutation(range(N))[:K]\n",
    "Z = X[:,p]\n",
    "# Z = np.random.randint(1, 6, (M,K)) \n",
    "\n",
    "#Q = Z.T @ Z\n",
    "#Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "R = X.T @ X\n",
    "Rt = torch.tensor(R,requires_grad=False).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "47234b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies both for eq. 7 and 8\n",
    "def error(a_i,Qt,qt):\n",
    "    return (0.5 * a_i.T @ Qt @ a_i) - (qt.T @ a_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef6089",
   "metadata": {},
   "source": [
    "## Defining the helper functions applyConstraints and furthestSum\n",
    "\n",
    "applyConstrants ensures that the constraints of the problem are upheld i.e. $e_{ij} \\geq 0$ and $1^T \\textbf{e}_j = 1$\n",
    "\n",
    "\n",
    "furthestSum is an effective way to initialize the values of $\\textbf{b}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ec2091d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 6 1 1 1 1 6 1 1 1 6 1 1 3 1 3]\n"
     ]
    }
   ],
   "source": [
    "def applyConstrains(M):\n",
    "    return softmax(M, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "#def furthestSum(X):\n",
    "    #idx = np.random.choice(range(0,N))\n",
    "    #x_j = X[:,idx]\n",
    "    #max_vals = [0]\n",
    "    #max_vals = max_vals * K\n",
    "\n",
    "    #b_j = np.empty((K,1))\n",
    "    #b_j_norm = np.empty((K,1))\n",
    "\n",
    "    #for i in range(N):\n",
    "    #    val = 0\n",
    "   #     for j in range(M):\n",
    "  #          val += np.abs(X[j, i] - x_j[j])\n",
    " #       max_idxs = [j for j, v in enumerate(max_vals) if val > v]\n",
    "#\n",
    "      #  if len(max_idxs) > 0:\n",
    "     #       max_vals[max_idxs[0]] = val\n",
    "    #\n",
    "    #b_j = b_j.astype(np.float64)\n",
    "            \n",
    "    #return b_j\n",
    "\n",
    "\n",
    "    \n",
    "# print(b_j)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b8101bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def furthestSum(X):\n",
    "    # Choose a random point for initialization\n",
    "    idx = int(np.random.choice(range(0,N)))\n",
    "    x_j = X[:,idx]\n",
    "\n",
    "    j_news = list()\n",
    "    j_news.append(idx)\n",
    "\n",
    "    excluded = [idx]\n",
    "\n",
    "    # Loop over the K archetypes\n",
    "    for n in range(K):\n",
    "        best_val = 0\n",
    "        best_idx = 0\n",
    "        # Loop over all unseen samples\n",
    "        for i in range(N-len(excluded)):\n",
    "            if i not in excluded:\n",
    "                val = 0\n",
    "                # sum over each element for each point\n",
    "                for ele in j_news:\n",
    "                    for j in range(M):\n",
    "                        \n",
    "                        val += np.abs(X[j,i] - X[j , ele])\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_idx = i\n",
    "                    \n",
    "        # \n",
    "        j_news.append(int(best_idx))\n",
    "        excluded.append(best_idx)\n",
    "        # Remove the random initialization\n",
    "        if n == 0:\n",
    "            j_news.pop(0)\n",
    "            excluded.pop(0)\n",
    "        \n",
    "    return j_news\n",
    "\n",
    "idxs = furthestSum(X)\n",
    "\n",
    "init_vals_b = X[init_idxs[i],:].astype(np.float64)\n",
    "init_vals_bt = torch.tensor(init_vals_b, requires_grad = False).float()\n",
    "\n",
    "init_vals_b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a273c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d3834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "62f0513f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "10\n",
      "25\n",
      "28\n",
      "83\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "14\n",
      "1\n",
      "2\n",
      "4\n",
      "12\n",
      "47\n",
      "73\n",
      "1\n",
      "2\n",
      "4\n",
      "12\n",
      "25\n",
      "47\n",
      "Loss before: tensor(295.9209, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(295.9209, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(4.1297, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(4.1297, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(302.6338, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(302.6338, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(197.2050, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(197.2050, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(13.3398, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(13.3398, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(53.1242, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.1242, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(79.0898, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.0898, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(222.3477, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(222.3477, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(260.7177, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(260.7177, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(120.1888, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(120.1888, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(24.4742, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(24.4742, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(170.0025, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(170.0025, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(161.9947, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(161.9947, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(3.5966, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.5966, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(23.1602, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(23.1602, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(151.2220, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(151.2220, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(117.2016, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(117.2016, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(41.7307, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.7307, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(151.3355, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(151.3355, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(72.8336, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(72.8336, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(39.1360, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(39.1360, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(75.4730, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(75.4730, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(173.4999, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(173.4999, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(21.6666, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.6666, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(5.8150, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(5.8150, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(69.0844, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(69.0844, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(232.0350, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(232.0350, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(45.4608, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.4608, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(53.4742, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(53.4742, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(300.9584, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(300.9584, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(33.9396, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.9396, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(109.1623, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(109.1623, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(225.3092, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(225.3092, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(29.7013, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.7013, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(61.2386, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.2386, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(27.1187, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(27.1187, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(213.8378, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(213.8378, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(89.6185, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(89.6185, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(234.5496, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(234.5496, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(186.6943, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(186.6943, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(0.3163, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(0.3163, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(38.1281, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.1281, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(29.5955, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.5955, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(138.9514, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(138.9514, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(50.8537, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.8537, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(169.5321, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(169.5321, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(311.6996, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(311.6996, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(124.8161, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(124.8161, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(30.2884, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(30.2884, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(45.9806, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(45.9806, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(109.9130, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(109.9130, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(113.1076, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(113.1076, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(130.9254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(130.9254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(85.9249, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(85.9249, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(403.7854, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(403.7854, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(20.5291, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.5291, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(41.1069, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(41.1069, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(141.6808, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(141.6808, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(59.4936, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(59.4936, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(57.3474, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.3474, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(38.4616, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(38.4616, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(44.4528, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(44.4528, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(318.7578, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(318.7578, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(437.3246, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(437.3246, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(2.9762, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.9762, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(70.0860, grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for a is tensor(70.0860, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(65.6009, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.6009, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(19.5793, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.5793, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(52.4331, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(52.4331, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(57.8545, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.8545, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(106.0209, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(106.0209, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(239.7072, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(239.7072, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(54.6731, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.6731, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(90.1512, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.1512, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(136.5138, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(136.5138, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(124.6324, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(124.6324, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(121.4115, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(121.4115, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(142.5585, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(142.5585, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(272.9309, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(272.9309, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(54.1615, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.1615, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(65.4043, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(65.4043, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(6.3218, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.3218, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(261.9630, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(261.9630, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(6.2028, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.2028, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(7.4254, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.4254, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(2.7959, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.7959, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(82.4412, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.4412, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(66.4715, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(66.4715, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(265.4465, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(265.4465, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(19.9887, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.9887, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(37.3418, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.3418, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(199.0059, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(199.0059, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(136.4457, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(136.4457, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(165.3780, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(165.3780, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(293.6978, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(293.6978, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(29.5224, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(29.5224, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(20.0616, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(20.0616, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(231.6035, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(231.6035, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(145.3464, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(145.3464, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(100.1912, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(100.1912, grad_fn=<NormBackward1>) at iteration 0\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n",
      "RSS at n=0 tensor(50.1627)\n",
      "Loss before: tensor(87.8450, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.8450, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(3.0741, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.0741, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(129.5322, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(129.5322, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(300.8932, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(300.8932, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(49.1032, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.1032, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(243.8943, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(243.8943, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(68.6886, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(68.6886, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(250.8999, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(250.8999, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(50.8824, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(50.8824, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(250.2910, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(250.2910, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(36.4833, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(36.4833, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(7.6584, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(7.6584, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(134.5258, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(134.5258, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(2.9187, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(2.9187, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(66.1607, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(66.1607, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(396.8502, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(396.8502, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(355.2598, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(355.2598, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.2646, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.2646, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(171.1030, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(171.1030, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(275.6140, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(275.6140, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(46.9224, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.9224, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(42.8376, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(42.8376, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(144.5877, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(144.5877, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(19.8125, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(19.8125, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(79.0913, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(79.0913, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(302.6833, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(302.6833, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(272.1864, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(272.1864, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(325.4481, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(325.4481, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(60.8303, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(60.8303, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(147.3581, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(147.3581, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(288.3975, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(288.3975, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(170.0264, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(170.0264, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(127.2993, grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for a is tensor(127.2993, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(54.0722, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(54.0722, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(97.1048, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(97.1048, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(183.2425, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(183.2425, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(159.5834, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(159.5834, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(1.5495, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(1.5495, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(71.7983, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(71.7983, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(545.3300, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(545.3300, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(100.4572, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(100.4572, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(245.1530, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(245.1530, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(469.1428, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(469.1428, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(18.8745, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(18.8745, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(121.5396, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(121.5396, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(217.1213, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(217.1213, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(367.4827, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(367.4827, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(109.5238, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(109.5238, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(16.1571, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.1571, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(92.1867, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(92.1867, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(3.5399, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(3.5399, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(343.0208, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(343.0208, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(43.9161, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(43.9161, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(21.8894, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(21.8894, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(221.5201, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(221.5201, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(56.2805, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(56.2805, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(28.8936, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(28.8936, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(57.2076, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(57.2076, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(67.7610, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.7610, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(441.0189, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(441.0189, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(150.4951, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(150.4951, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(93.9576, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(93.9576, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(147.4243, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(147.4243, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(37.4733, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(37.4733, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(90.4604, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(90.4604, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(33.0325, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(33.0325, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(22.6449, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(22.6449, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(32.2083, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(32.2083, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(173.4322, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(173.4322, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.6522, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.6522, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(179.0471, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(179.0471, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(236.3672, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(236.3672, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(87.8705, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(87.8705, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(82.4685, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(82.4685, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(215.5094, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(215.5094, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(110.9639, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(110.9639, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(81.0522, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(81.0522, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(363.6491, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(363.6491, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(156.8893, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(156.8893, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(67.6287, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(67.6287, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(46.6398, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.6398, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(449.1877, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(449.1877, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(61.3327, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(61.3327, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(165.0883, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(165.0883, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(46.6541, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(46.6541, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(122.3488, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(122.3488, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(316.0958, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(316.0958, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(341.8664, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(341.8664, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(332.6489, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(332.6489, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(16.8059, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(16.8059, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(118.3992, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(118.3992, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(6.1557, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(6.1557, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(427.0618, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(427.0618, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(99.4015, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(99.4015, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(250.4737, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(250.4737, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(49.1341, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(49.1341, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(251.9300, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(251.9300, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(35.8261, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(35.8261, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(34.2444, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(34.2444, grad_fn=<NormBackward1>) at iteration 0\n",
      "Loss before: tensor(8.2300, grad_fn=<NormBackward1>)\n",
      "Loss for a is tensor(8.2300, grad_fn=<NormBackward1>) at iteration 0\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35372/781309662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# TRAINING LOOP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 100000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mDelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35372/4071995011.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(a_i, Qt, qt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# applies both for eq. 7 and 8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mQt\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0ma_i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mqt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0ma_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "RSS_values = list()\n",
    "A = np.zeros((K,N))#.tolist()\n",
    "B = np.zeros((N,K))#.tolist()\n",
    "\n",
    "# Get initial indexes for B via. furthest sum procedure\n",
    "init_idxs = furthestSum(X)\n",
    "\n",
    "\n",
    "\n",
    "# LOOP UNTIL RSS IS LOW\n",
    "for n in range(2): #N\n",
    "    Q = Z.T @ Z\n",
    "    Qt = torch.tensor(Q,requires_grad=False).float()\n",
    "\n",
    "    # LOOP THROUGH ENTIRE A\n",
    "    for i in range(N):\n",
    "        q = Z.T @ X[:,i]\n",
    "        qt = torch.tensor(q,requires_grad=False).float()\n",
    "        \n",
    "        a_i = torch.autograd.Variable(torch.rand(K, 1), requires_grad=True) # eller er det Kx1 ?\n",
    "        \n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(a_i,Qt,qt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(100): # 100000\n",
    "            Delta = error(a_i,Qt,qt) \n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            a_i.data -= step_size * a_i.grad.data # step\n",
    "            a_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss for a is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "        \n",
    "        A[:,i] = np.array(a_i.tolist()).flatten() \n",
    "        \n",
    "        # print('Loss after: %s' % (torch.norm( error(a_i,Qt,qt) )))\n",
    "\n",
    "    ### apply softmax here? ###\n",
    "    \n",
    "    A = applyConstrains(A)\n",
    "    Z = X @ A.T @ np.linalg.inv(A@A.T)\n",
    "    \n",
    "    \n",
    "    # LOOP THROUGH ENTIRE B\n",
    "    for i in range(K): #K\n",
    "        r = X.T @ Z[:,i]\n",
    "        rt = torch.tensor(r,requires_grad=False).float()\n",
    "        \n",
    "        \n",
    "        b_i = torch.autograd.Variable(torch.randn(N,1), requires_grad=True)\n",
    "        \n",
    "        \n",
    "        stop_loss = 1e-6\n",
    "        step_size = 0.00001 # stop_loss / 3.0\n",
    "\n",
    "        err = error(b_i,Rt,rt)\n",
    "        print('Loss before: %s' % (torch.norm( err, p=2)))\n",
    "\n",
    "        # TRAINING LOOP\n",
    "        for k in range(10000): # 100000\n",
    "            Delta = error(b_i,Rt,rt)\n",
    "            L = torch.norm(Delta, p=2)\n",
    "            L.backward()\n",
    "            b_i.data -= step_size * b_i.grad.data # step\n",
    "            b_i.grad.data.zero_()\n",
    "            if k % 10000 == 0: print('Loss for b is %s at iteration %i' % (L, k))\n",
    "            if abs(L) < stop_loss:\n",
    "                print('It took %s iterations to achieve %s loss.' % (k, step_size))\n",
    "                break\n",
    "\n",
    "        B[:,i] = np.array(b_i.tolist()).flatten() \n",
    "        print('Loss after: %s' % (torch.norm( error(b_i,Rt,rt) )))    \n",
    "    \n",
    "    # apply softmax here\n",
    "    B = applyConstrains(B)\n",
    "    Z = X @ B\n",
    "    \n",
    "    Zt = torch.tensor(Z, requires_grad=False).float()\n",
    "    At = torch.tensor(A, requires_grad=False).float()\n",
    "    Xt = torch.tensor(X,requires_grad=False).float()\n",
    "    print(\"RSS at n=%s\" % n, torch.norm(Xt-Zt@At,p='fro'))\n",
    "    RSS_values.append( torch.norm(Xt-Zt@At,p='fro'))\n",
    "    \n",
    "end = timer()\n",
    "\n",
    "print(\"It took: {0} seconds to finish running\".format(end - start))\n",
    "print(\"The best RSS value was\", min(RSS_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adbd8a",
   "metadata": {},
   "source": [
    "### Time at different data sizes\n",
    "### Here we run 1000 inner loops and 10 in the outer\n",
    "#### For X = 3 x 17  \n",
    "##### It took: 17.8 seconds\n",
    "\n",
    "\n",
    "#### For X = 6 x 17  \n",
    "##### It took: 28.08 seconds\n",
    "\n",
    "\n",
    "#### For X = 12 x 17\n",
    "#### It took: 48 seconds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2cfbdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def furthestSum(X):\n",
    "    idx = np.random.choice(range(0,N))\n",
    "    x_j = X[:,idx]\n",
    "    max_vals = [0]\n",
    "    max_vals = max_vals * K\n",
    "\n",
    "    b_j = np.empty((K,1))\n",
    "    b_j_norm = np.empty((K,1))\n",
    "\n",
    "    for i in range(N):\n",
    "        val = 0\n",
    "        for j in range(M):\n",
    "            val += np.abs(X[j, i] - x_j[j])\n",
    "        max_idxs = [j for j, v in enumerate(max_vals) if val > v]\n",
    "\n",
    "        if len(max_idxs) > 0:\n",
    "            max_vals[max_idxs[0]] = val\n",
    "            b_j = X[:, i]\n",
    "            \n",
    "    return b_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4c21245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 0, 14, 73, 47]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random point for initialization\n",
    "idx = int(np.random.choice(range(0,N)))\n",
    "x_j = X[:,idx]\n",
    "\n",
    "j_news = list()\n",
    "j_news.append(idx)\n",
    "\n",
    "excluded = [idx]\n",
    "\n",
    "# Loop over the K archetypes\n",
    "for n in range(K):\n",
    "    best_val = 0\n",
    "    best_idx = 0\n",
    "    # Loop over all unseen samples\n",
    "    for i in range(N-len(excluded)):\n",
    "        if i not in excluded:\n",
    "            val = 0\n",
    "            # sum over each element for each point\n",
    "            for ele in j_news:\n",
    "                for j in range(M):\n",
    "                    val += np.abs(X[j,i] - X[j , ele])\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_idx = i\n",
    "    \n",
    "    # \n",
    "    j_news.append(best_idx)\n",
    "    excluded.append(best_idx)\n",
    "    # Remove the random initialization\n",
    "    if n == 0:\n",
    "        j_news.pop(0)\n",
    "        excluded.pop(0)\n",
    "        \n",
    "    \n",
    "j_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0857a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419dfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
