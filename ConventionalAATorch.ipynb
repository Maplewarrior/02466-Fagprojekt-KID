{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "201dd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random \n",
    "from scipy.special import softmax\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f6a726c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ESS8_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c690486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SD1', 'PO1', 'UN1', 'AC1', 'SC1',\n",
    "       'ST1', 'CO1', 'UN2', 'TR1', 'HD1', 'SD2','BE1','AC2', 'SC2', 'ST2',\n",
    "       'CO2', 'PO2', 'BE2', 'UN3', 'TR2','HD2']].iloc[range(100),:]\n",
    "X = X.to_numpy().T\n",
    "\n",
    "# Shapes of data:\n",
    "N, M = X.T.shape\n",
    "\n",
    "# Number of archetypes\n",
    "K = 5\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5cb78fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined RSS loss function, input are tensors\n",
    "def error(X,B,A):\n",
    "    return torch.norm(X - X@B@A, p='fro')**2\n",
    "\n",
    "\n",
    "def applyConstraints(A):\n",
    "    m = nn.Softmax(dim=0)\n",
    "    return m(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff7a0c",
   "metadata": {},
   "source": [
    "## Convetional AA\n",
    "\n",
    "#### Dimensions:\n",
    "\n",
    "$\\textbf{X}$ = $\\textbf{M}$ x $\\textbf{N}$\n",
    "\n",
    "$\\textbf{B}$ = $\\textbf{N}$ x $\\textbf{K}$\n",
    "\n",
    "$\\textbf{A}$ = $\\textbf{K}$ x $\\textbf{N}$\n",
    "\n",
    "\n",
    "hvor: \n",
    "\n",
    "$\\textbf{Z}$ = $\\textbf{X} \\textbf{B}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fc4ca846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = X[:,random.sample(range(1, M), K)]\n",
    "#print(Z.shape)\n",
    "\n",
    "\n",
    "Xt = torch.tensor(X,requires_grad=False).float()\n",
    "A = torch.autograd.Variable(torch.rand(K, N), requires_grad=True)\n",
    "B = torch.autograd.Variable(torch.rand(N, K), requires_grad=True)\n",
    "\n",
    "\n",
    "    \n",
    "optimizer = optim.Adam([A, B], amsgrad = True) #, lr = 0.01)\n",
    "\n",
    "\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad()\n",
    "    L = error(Xt, applyConstraints(B), applyConstraints(A))\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "A = applyConstraints(A)\n",
    "B = applyConstraints(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1aae3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<AddBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], grad_fn=<AddBackward0>)\n",
      "tensor(0.4744, grad_fn=<UnbindBackward0>)\n",
      "tensor(3.6037e-05, grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sum(B))\n",
    "print(sum(A))\n",
    "\n",
    "\n",
    "print(max(B.flatten()))\n",
    "#print(np.mean(np.array(B.flatten())))\n",
    "print(min(B.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d2e35985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AA:\n",
    "    \"\"\"\n",
    "    Class for applying conventional archetypal analysis.\n",
    "    \n",
    "    Input: \n",
    "    X = M x N array of data (features x samples)'\n",
    "    \"\"\"\n",
    "    A = None\n",
    "    B = None\n",
    "    RSS = None\n",
    "    done = False\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        self.M, self.N = X.shape\n",
    "        self.X = X\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _error(self, X,B,A):   \n",
    "        return torch.norm(X - X@B@A, p='fro')**2\n",
    "\n",
    "    def _applyConstraints(A):    \n",
    "        m = nn.Softmax(dim=0)\n",
    "        return m(A)\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for applying Archetypal Analysis\n",
    "    -----------------------------------------\n",
    "        Input:\n",
    "                K: number of archetypes\n",
    "                n_iter: Number of iterations\n",
    "                lr: Learning rate\n",
    "                time: Boolean, if true the time until convergence is tracked.\n",
    "                \n",
    "    \"\"\"\n",
    "    def computeArchetypes(self, K, n_iter, lr = None, time = True):\n",
    "        if time == True:\n",
    "            start = timer()\n",
    "        \n",
    "        Xt = torch.tensor(self.X,requires_grad=False).float()\n",
    "        A = torch.autograd.Variable(torch.rand(K, self.N), requires_grad=True)\n",
    "        B = torch.autograd.Variable(torch.rand(self.N, K), requires_grad=True)\n",
    "        \n",
    "        optimizer = optim.Adam([A, B], amsgrad = True) #, lr = 0.01)\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            optimizer.zero_grad()\n",
    "            L = error(Xt, applyConstraints(B), applyConstraints(A))\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        A = applyConstraints(A)\n",
    "        B = applyConstraints(B)\n",
    "        \n",
    "        if time == True:\n",
    "            end = timer()\n",
    "            print(\"It took \", end-start, \" seconds to converge :)\")\n",
    "        \n",
    "        # Compute the final error\n",
    "        RSS = error(Xt, B, A)\n",
    "        print(\"The final RSS was: \", RSS)\n",
    "        \n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.RSS = RSS\n",
    "        self.Z = Xt @ B\n",
    "        \n",
    "        done = True\n",
    "        print(\"Sucessfully computed the archetypes\")\n",
    "        \n",
    "        \n",
    "    def getA(self):\n",
    "        return self.A\n",
    "\n",
    "    def getB(self):\n",
    "        return self.B\n",
    "    \n",
    "    def getZ(self):\n",
    "        return self.Z\n",
    "    \n",
    "    def getRSS(self):\n",
    "        return self.RSS\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9c2b8",
   "metadata": {},
   "source": [
    "# Ordinal AA\n",
    "\n",
    "#### See the article: \"Gaussian Processes for Ordinal Regression\"\n",
    "\n",
    "##### Gaussian kernel, prior and likelihood function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "da1a2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9026754320582246e-133\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Mercer kernel function\n",
    "# Input: the hyperparameter k and two data matrices of dimensions M x N\n",
    "def gaussianKernel(k, x, y):\n",
    "    sqdist = cdist(x.T, y.T, 'sqeuclidean')\n",
    "    return np.exp(-(k/2)*sqdist)\n",
    "\n",
    "\n",
    "Sigma = gaussianKernel(2, X, X)\n",
    "\n",
    "\n",
    "\n",
    "def prior(f, Sigma):\n",
    "    Zf = (2*np.pi)**(len(Sigma)/2) * (np.linalg.norm(Sigma)**(1/2))\n",
    "    return 1/Zf * np.exp(-1/2 * f.T @ Sigma @ f)\n",
    "\n",
    "test = prior(X.T, Sigma)\n",
    "print(max(test.flatten()))\n",
    "# Very small values... this is due to Zf being VERY large.\n",
    "## what is exactly meant by |Sigma| in the .pdf? --> assuming norm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9e92126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010001899486276224\n",
      "0.012453964655117434\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(Sigma.flatten()))\n",
    "\n",
    "print(np.percentile(Sigma.flatten(), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "0a93ac67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.67"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = np.mean(X)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03215db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
