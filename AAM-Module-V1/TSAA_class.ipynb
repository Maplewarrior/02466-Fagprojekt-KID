{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91977d14",
   "metadata": {},
   "source": [
    "# This script carries out ordinal archetypal analysis using a two step procedure. \n",
    "### The method was proposed by et. al. Epifanio in \"Archetypal Analysis for Ordinal Data\"\n",
    "\n",
    "\n",
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8d36ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from CAA_class import _CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9dd47",
   "metadata": {},
   "source": [
    "# Load the dataset and remove the nominal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12837fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ESS8_data.csv')\n",
    "attributeNames = df.columns\n",
    "\n",
    "X = df.drop(['ID', 'Country', 'left_right', 'immigrants', 'homosexual',\n",
    "       'Satisfy_life', 'Satisfy_health', 'Attitude2EU', 'gender', 'age',\n",
    "       'education', 'subjective_income', 'TR_CFA', 'CO_CFA', 'SC_CFA',\n",
    "       'HD_CFA', 'ST_CFA', 'SD_CFA', 'PO_CFA', 'AC_CFA', 'UN_CFA', 'BE_CFA',\n",
    "       'SBM8', 'SBM9', 'SBM16', 'LCA5', 'LCA8', 'LCA16'], axis = 1).to_numpy()\n",
    "\n",
    "N, M = X.shape\n",
    "\n",
    "#print(N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48e250",
   "metadata": {},
   "source": [
    "### Calculate the $\\theta $ values. \n",
    "### Replace observations in X with the corresponding $\\theta_i$\n",
    "### Apply conventional AA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b96ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TSAA:\n",
    "    \n",
    "    RSS = []    \n",
    "    \n",
    "    \n",
    "    def _logOdds(self, X):\n",
    "        Ordinals = range(min(self.X.flatten()), max(self.X.flatten()+1))\n",
    "    \n",
    "        probs = [(np.count_nonzero(self.X.flatten() == e))/len(self.X.flatten()) for e in Ordinals]\n",
    "        baseline = max(probs)\n",
    "    \n",
    "        logvals = [np.log(probs[i]/baseline) for i in range(len(probs))]\n",
    "        return logvals\n",
    "    \n",
    "    def _applySoftmax(self,M):\n",
    "        return softmax(M)\n",
    "    \n",
    "    \n",
    "    def _projectOrdinals(self, X):\n",
    "        M, N = X.shape\n",
    "        \n",
    "        X_thilde = np.empty((M, N))\n",
    "        \n",
    "        theta = self._applySoftmax(self._logOdds(X))\n",
    "        Ordinals = range(min(self.X.flatten()), max(self.X.flatten()+1))\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                idx = X[i,j]-1\n",
    "                X_thilde[i,j] = theta[idx]\n",
    "                \n",
    "        return X_thilde\n",
    "\n",
    "    \n",
    "    def _error(self, X,B,A):\n",
    "        return torch.norm(X - X@B@A, p='fro')**2\n",
    "    \n",
    "    def _apply_constraints(self, A):\n",
    "        m = nn.Softmax(dim=0)\n",
    "        return m(A)\n",
    "    \n",
    "    \n",
    "    ############# Two-step ordinal AA #############\n",
    "    def _compute_archetypes(self, X, K, n_iter, lr, mute,columns):\n",
    "        \n",
    "        ##### Project the data #####\n",
    "        X = self._projectOrdinals(X)\n",
    "        \n",
    "        \n",
    "        ##### Now conventional AA is applied #####\n",
    "        self.RSS = []\n",
    "        start = timer()\n",
    "        if not mute:\n",
    "            loading_bar = _loading_bar(n_iter, \"Conventional Arhcetypal Analysis\")\n",
    "        N, _ = X.T.shape\n",
    "        Xt = torch.tensor(X,requires_grad=False).float()\n",
    "        A = torch.autograd.Variable(torch.rand(K, N), requires_grad=True)\n",
    "        B = torch.autograd.Variable(torch.rand(N, K), requires_grad=True)\n",
    "        optimizer = optim.Adam([A, B], amsgrad = True, lr = 0.01)\n",
    "\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            if not mute:\n",
    "                loading_bar._update()\n",
    "            optimizer.zero_grad()\n",
    "            L = self._error(Xt, self._apply_constraints(B), self._apply_constraints(A))\n",
    "            self.RSS.append(L.detach().numpy())\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        A_f = self._apply_constraints(A).detach().numpy()\n",
    "        B_f = self._apply_constraints(B).detach().numpy()\n",
    "        Z_f = (Xt@self._apply_constraints(B)).detach().numpy()\n",
    "        X_hat_f = X@B_f@A_f\n",
    "        end = timer()\n",
    "        time = round(end-start,2)\n",
    "    \n",
    "        ########## POST ANALYSIS ##########\n",
    "        A_f = self._apply_constraints(A).detach().numpy()\n",
    "        B_f = self._apply_constraints(B).detach().numpy()\n",
    "        Z_f = (Xt@self._apply_constraints(B)).detach().numpy()\n",
    "        X_hat_f = X@B_f@A_f\n",
    "        end = timer()\n",
    "        time = round(end-start,2)\n",
    "        \n",
    "        result = _CAA_result(A_f, B_f, X, X_hat_f, n_iter, self.RSS, Z_f, K, time,columns,\"TSAA\")\n",
    "        \n",
    "        \n",
    "        if not mute:\n",
    "            result._print()\n",
    "\n",
    "        return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f221ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
